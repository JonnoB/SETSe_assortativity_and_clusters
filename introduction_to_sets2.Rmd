---
title: "Untitled"
author: "Jonathan Bourne"
date: "09/03/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---
https://www.quantamagazine.org/the-thoughts-of-a-spiderweb-20170523/
Hilton Japyassu UFBA Brazil
Fritz Vollrath -arachnologist Oxford

Deep graph infomax on stellargraph

repo on some interesting road safety network data
https://github.com/Moradii/stlnpp
The paper
https://doi.org/10.1016/j.spasta.2019.100400

  

#Set up
```{r Setup}

packages <- c("tidyverse", "igraph","readr","readxl", "broom", "stringr", "rlang", "latex2exp", "minpack.lm", "ggraph", "patchwork", "rsample", "VGAM", "class", "lubridate", "caret", "cowplot", "yardstick")

new.packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

select <- dplyr::select
arrange <- dplyr::arrange
map <- purrr::map
sapply(packages, library, character.only = TRUE)

library(rSETSe)

#Set up file system to read the correct folders this switches between aws and windows mode

#basewd <- "/home/jonno/Dropbox/Jonathan_Bourne_Phd_Folder"
LatexFolder <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Sets Paper 1" 
FiguresFolder <- file.path(LatexFolder, "Figures")
TablesFolder <- file.path(LatexFolder, "Tables")
MatricesFolder <- file.path(LatexFolder, "Matrices")
PLwd <- "/home/jonno/setse_1_data"
CodeFolder <- "/home/jonno/SETSe_assortativity_and_clusters"
SubcodeFolder <- file.path(CodeFolder, "sub_code")

#Load some other useful functions
list.files("/home/jonno/Useful_PhD__R_Functions", pattern = ".R", full.names = T) %>%
  walk(~source(.x))


```

#4 node 3 edge
provides the elevation position of the nodes in the opening example
```{r}

example_g <-graph_from_data_frame(tibble(from=c("A", "B", "B"), to = c("B", "C", "D")),
              directed = FALSE,
              vertices = tibble(node = c("A", "B", "C", "D"), node_name = node, force = c(2L,0L,-1L,-1L)))

example_df <- example_g %>%
  prepare_SETSe_continuous(., node_names = "node_name", k = 1000, force_var = "force") %>% 
  auto_SETSe()

example_df$node_embeddings %>% pull(elevation) %>% round(., 4)



```


#Peels quintet

This section re-creates the networks used in Peel et al

```{r}

source(file = file.path(SubcodeFolder, "prepare_peels_quintet_data.R"))

 (plot_list[[1]] | plot_list[[2]] | plot_list[[3]] )/
     (plot_list[[4]]| plot_list[[5]]) +
   plot_annotation(
  title = "The Peels quintet of assortativity identical graphs",
#  subtitle = 'The graphs can be separated using SETSe'
)
ggsave(file.path(FiguresFolder,  "Peels_quintet.pdf"))

rm(plot_list)
 

```

##Peel Strain

```{r}
#requires the "quintet" and "class_data_df" data frames
source(file = file.path(SubcodeFolder, "process_peel_strain.R"))
aggregated_peels <- process_comparison_methods_aggregated()
#the basic version used the original data

aggregated_peels$agg  %>%
  group_by(embeddings_method) %>%
  filter(embeddings_method != "SDNE") %>%
  mutate(dimension_1 = (dimension_1-min(dimension_1))/(max(dimension_1)-min(dimension_1)),
         dimension_2 = (dimension_2-min(dimension_2))/(max(dimension_2)-min(dimension_2))) %>%
  ggplot(aes(x = dimension_1, y = dimension_2, colour = type)) + 
  geom_point() +
  facet_wrap(~embeddings_method) +
  labs(title = "Separating Peel's Quintet in two dimensions",
       x = "first dimension (tension for SETSe)",
       y = "second dimension (elevation for SETSe)")
ggsave(file.path(FiguresFolder, "Seperating_Peels_quintet.pdf"))


aggregated_peels$full %>%
  filter(.metric =="accuracy") %>%
  mutate(type2 = paste0("Type ", type,": ", model_type, " detection") %>% str_replace(., "_", " ")) %>%
  ggplot(aes(x = embeddings_method, y = .estimate, fill = embeddings_method)) + geom_boxplot()+
  facet_wrap(~type2)  +
  guides(fill=FALSE) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Comparison of graph embeddings methods for class identification on Peel's Quintet",
       y = "accuracy",
       x = "")
ggsave(file.path(FiguresFolder,  "peels_embedding_linear_sep.pdf"))   

```

##Peel knn


THIS SECTION IS NOT NECCESSARY AS THE graphs are linearly seperable.

This tests the accuracy of the classifier created by a knn and a multinomial logistic regression.
It shows that strain and elevation can accurately classify the graph types

accuract of the knn is about 83% and 84%. N.B the old data was higher (95% and 97%), but group B had a massive strain spread which was probably an error.%
```{r}
# #This needs to use some save files becuase it takes ages
# source(file = file.path(SubcodeFolder, "model_basic_peels.R"))
# 
# #Accuracy is between 95% and 96% for all values of k between 1 and ten
# nearest_k_results %>%
#   ggplot(aes(x = k, y = accuracy )) + 
#   geom_line() +
#   labs(title = "The mean accuracy for 100 repeats of 10 fold cross validation for knn classifier", 
#        y = "mean accuracy",
#        x ="nearest neighbours")
# #this file path is for casa presentation. delete after
# ggsave(file.path(FiguresFolder, "knn_accuracy.pdf"))


```

##Variable k
If the strength of relationship within and between groups varies then we findd that SETSe can distinguish between topologically identical networks that differ only in the strength of relationship.
```{r}
##This is section is not necessary for the paper
# 
# peel_strain_sum %>%
#   ggplot(., aes( x= mean_tension_per_node, y = mean_abs_elevation, colour = as.factor(k))) + geom_point(alpha = 0.5) +
#   facet_wrap(~graph_type, scales = "free_y") +
#   labs(title = "The effect of different relationship strengths within and between groups on \ntension and elevation",
#        colour = "relationship"
#        )
# ggsave(file.path(FiguresFolder, "strength_effect.pdf"))
# 
# peel_strain_sum %>%
#   filter(
#   ) %>%
#   ggplot(., aes( x= mean_tension_per_node, y = mean_abs_elevation, colour = as.factor(k))) + geom_point(alpha = 0.5) +
#   facet_wrap(~graph_type, scales = "free_y")
# 
# 
# 
# peel_strain_sum %>%
#   ggplot(., aes( x= mean_tension_per_node, y = mean_abs_elevation, colour = as.factor(graph_type))) + geom_point(alpha = 0.5) +
#   facet_wrap(~factor(k), scales = "free_y")

```


##The position of individual nodes 

This chunk shows that individual nodes naturally cluster by elevation with thier own sub groups sometimes the tension is also needed. I am not sure how to extract this without using a model though
```{r}
#There is possibly a problem with the node position code. The part that created detected_communities is producing 40k lines intead of 20k, this may or may not be a problem...

#....The 40k lines are becuase both groups a and be are checked.

source(file = file.path(SubcodeFolder, "node_position.R"))
 
#Use ARI to compare the quality of all clustering techniques
combos_df <- expand_grid(names = detected_communities %>% select(contains("clustering")) %>% names, graph_id = unique(detected_communities$graph_id))

test_ARI <-1:nrow(combos_df) %>%
  map_df(~{
    print(.x)
    temp <-detected_communities %>%
      filter(graph_id == combos_df$graph_id[.x])
    
    tibble(
      name = combos_df$names[.x],
      graph_id = combos_df$graph_id[.x],
      graph_class = unique(temp$graph_class),
      ARI = adjustedRandIndex(temp$sub_class, temp %>% pull(combos_df$names[.x]) ))
    
  })

#plot the results
#The ability to cluster graphs that have communities or groups that are not defined by their linking to each other greatly outstrips everything else

#plot and example of the true communities
detected_communities %>%
  mutate(sub_class = str_replace(sub_class, "_", " "),
         graph_class = paste("Type", graph_class)) %>%
  filter(graph_id %in% seq(100, 500, by = 100)) %>%
  ggplot(aes(x = tension_mean_range, y = elevation_range, colour = sub_class)) + geom_point() +
  facet_wrap(~graph_class) +
  labs(title = "The position of individual nodes in each graph type of Peel's Quintet",
       x = "node tension",
       y = "node elevation",
       color = "sub class") +
  theme(legend.position = c(0.9, 0.1), legend.justification = c(1, 0)) #positions the legend in the middle of the 6th panel
ggsave(file.path(FiguresFolder,  "peels_quintet_groups_in_setse_space.pdf"))   


 #plot showing how SETSe does the comparisons
 detected_communities %>%
    filter(graph_id %in% seq(100, 500, by = 100)) %>%
  ggplot(aes(x = tension_mean, y = elevation, colour = factor(clustering_kmeans))) + geom_point() +
  facet_wrap(~graph_class, scales = "free_y") +
   labs(title = "the position of each node in a graph coloured by kmeans groups",
        colour = "kmeans cluster")

              
        #when using knowlegde of the groups
        test_ARI %>%
          filter(!grepl("full", name)) %>%
          mutate(clustering = name %>% str_remove(., "clustering_") %>% str_replace(., "_", " "),
                 clustering = ifelse(clustering =="kmeans", "SETSe kmeans", clustering)) %>%
          ggplot(aes(x = clustering, y = ARI, fill = clustering)) + geom_boxplot() +
          facet_wrap(~graph_class) +
          labs(title = "Adjusted rand index for different clustering methods, for each graph type",
               x = "Clustering method") + 
          theme(
            axis.text.x=element_blank(),
            axis.ticks.x=element_blank())
    ggsave(file.path(FiguresFolder,  "peels_clustering_ari.pdf"))        
        
         #when not using knowledge of the groups
         test_ARI %>%
          filter(grepl("full", name))  %>%
          mutate(clustering = name %>% str_remove(., "clustering_") %>% str_replace(., "_", " "),
                 clustering = ifelse(clustering =="kmeans", "SETSe kmeans", clustering)) %>%
          ggplot(aes(x = clustering, y = ARI, fill = clustering)) + geom_boxplot() +
          facet_wrap(~graph_class) +
          labs(title = "Adjusted rand index for different clustering methods, for each graph type")

         #
 test <- test_ARI %>%
    group_by(name, graph_class) %>%
    summarise(mean = mean(ARI))
```


#Facebook data

In this section I analyse the facebook 100 dataset. I got this from https://archive.org/details/oxford-2005-facebook-matrix

delete all nodes that graduated before 2004 and after 2009
keep only student types 1 and 2

normalise edge force to some fixed value e.g. 0.001 per edge


This is a paper on a facebook study that went a bit far and the data had to be removed.
worth commenting on probably
https://www-sciencedirect-com.libproxy.ucl.ac.uk/science/article/pii/S0378873308000385
```{r}

   g_df <- readRDS(uni_files[1]) %>% as_data_frame(., what = "vertices")

#convert the facebook .mat files to igraph format and save as rds
#this is commented out as it takes a long time and checking to see if the files are generated has not been done.
#source(file = file.path(SubcodeFolder, "convert_facebook_files.R"))

uni_files <- list.files("/home/jonno/setse_1_data/facebook100/facebook100_igraph", full.names = T)


#Create the biconnected data for the graphs with the time it takes to calculate.
source(file = file.path(SubcodeFolder, "facebook_biconnected.R"))

#This data frame shows that all uni's have a connected component of over 99%
#the smaller components can be disgarded
source(file = file.path(SubcodeFolder, "create_facebook_uni_stats.R"))

uni_stats %>%
  summarise(nodes = sum(max_component),
            student_type_1 = sum(student_type_1),
            student_type_2 =sum(student_type_2)) %>%
  mutate(perc_1 = student_type_1/nodes,
         perc_2 = student_type_2/nodes)

#I will use dorms over 0.5% of the total known university populations
#I will use years with over 1% of the population the rest will be reset to the mean of the students to not exert force

#In the one vs all use the height of the node when it is postive force and not when it isn't
#This is useful when you want to look at the distribution of the students.

#alternative use the euclidean distance of the mean of all positive force for the uni as a whole.
#This can be combined with the euclidean distance of the mean tension for a two dimensional projection

test2 <- left_join(uni_stats, biconnected_data) %>% as_tibble() %>%
  mutate(max_size_ratio = max_size/nodes) %>%
  select(uni_name:edges, total:max_size_ratio, density) %>%
  select(uni_name:nodes, max_size, max_size_ratio, everything()) %>%
  mutate(time_node = time/nodes)

#This creates year embeddings for all the facebook data. 
#It is commented out as the code is does not check if the file has been made
#source(file = file.path(SubcodeFolder, "facebook_embeddings_year.R"))
#source(file = file.path(SubcodeFolder, "process_facebook_embeddings.R"))


process_facebook_embeddings(file.path(PLwd, "facebook_embeddings" ,"facebook"), 
                            file.path(PLwd, "facebook_embeddings" , "processed_embeddings"))
process_facebook_embeddings(file.path(PLwd, "facebook_embeddings", "facebookauto"), 
                            file.path(PLwd, "facebook_embeddings", "processed_embeddings_auto"))

process_facebook_embeddings(file.path(PLwd, "facebook_embeddings", "facebook1000"), 
                            file.path(PLwd, "facebook_embeddings", "processed_embeddings_1000"))

```


##100 tension elevation
This plots the tension and elevation for all the uni's in the facebook
```{r}

#creates/loads the embedded_assort dataframe which contains the mean embeddings of all the methods across the facebook dataset
source(file = file.path(SubcodeFolder, "create_assort_dataframe.R"))

#plot the facebook uni's by elevation and tension
#There is no relationship with size of uni which is good.
#there is a relationship between tension and elevation, which is less good I guess
#question is how does this relate to student housing 

#highlighting auburb main and michigen is a good idea to show how the assortativity of similar uni's is very separate
relate_figure <-  embedded_assort %>%
  group_by(model) %>%
  mutate(X = mean_abs_X2,
         X = (X-mean(X))/sd(X),
         Y = mean_abs_X1,
         Y = (Y-mean(Y))/sd(Y)) %>%
  #  filter(mean_tension<1000) %>%
  ggplot(aes(y = X, x = Y, colour = year_assort)) + geom_point() + 
  scale_color_viridis_c() +
  facet_wrap(~model, scales = "free")+
  labs(title = "The normalised average absolute embeddings of the Facebook 100 dataset.",
       x = "first dimension (tension for SETSe)",
       y = "second dimension (elevation for SETSe)",
       color = "year\nassortativity")  +
  theme(legend.position = c(0.9, 0.1), legend.justification = c(1, 0)) #positions the legend in the middle of the 6th panel

relate_figure 
ggsave(file.path(FiguresFolder, "facebook_100_elev_tens.pdf"))

ggplotly(relate_figure)

assort_embeds_perf <- unique(embedded_assort$model) %>%
  map_df(~{
    
    data_df <- embedded_assort %>% filter(model ==.x)
    
      #the year assortativity of the model can be predicted by the below linear model
  X2_only <-lm(year_assort ~ mean_abs_X2 , data_df) %>% glance() 
  X1_only <- lm(year_assort ~ mean_abs_X1 , data_df) %>% glance()
  both_mod <- lm(year_assort ~ mean_abs_X2 + mean_abs_X1, data_df) %>% glance
  
  bind_rows(X2_only %>% mutate(variables_used = "X2"),
            X1_only %>% mutate(variables_used = "X1"),
            both_mod %>% mutate(variables_used = "both")) %>%
  mutate(model = .x)
    
  }) 

#SETSe is much better than the others at producing meaningful embeddings
#node2vec gets a 20 point bump if the model uses a square term
assort_embeds_perf %>% filter(variables_used =="both") %>% arrange(-r.squared) %>%
  select(model, R = r.squared, p.value)

```

##time memory complexity

This chunk shows that the iteration time complexity and the network memory demands are linear with the number of edges in the network.

```{r}

fb_network_dynamics <- readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/processed_embeddings", "facebook_network_dynamics.rds" ))

fb_network_dynamics_auto <- readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/processed_embeddings", "facebook_network_dynamics.rds" ))

#This is interesting but not very useful
  # fb_network_dynamics %>%
  #   left_join(uni_stats, by = c("file_name")) %>%
  # ggplot(aes(x = Iter, y = log10(static_force))) + geom_jitter()


#the time taken for an iteration is linear with the product of edges and nodes

  time_taken_df <- readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/processed_embeddings", "facebook_time_taken.rds" )) %>%
    mutate(time_taken = as.numeric(time_diff)) %>%
    left_join(., 
              fb_network_dynamics %>%
  bind_rows() %>%
    group_by(file_name) %>%
    summarise_all(last)
  ) %>%
    filter(nodes>=100) %>%
    mutate(time_per_iter = time_taken/Iter,
           node_x_edge = nodes*edges,
           node_x_edge2 = sqrt(nodes*edges),
           log_node_x_edge = log(node_x_edge),
           log_time_per_iter = log(time_per_iter),
           log_time_taken = log(time_taken),
           log_edges = log(edges))
  
time_taken_df_auto <- readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/processed_embeddings_auto", "facebook_time_taken.rds" )) %>%
    mutate(time_taken = as.numeric(time_diff)) %>%
    left_join(., 
              fb_network_dynamics %>%
  bind_rows() %>%
    group_by(file_name) %>%
    summarise_all(last)
  ) %>%
    filter(nodes>=100) %>%
    mutate(time_per_iter = time_taken/Iter,
           node_x_edge = nodes*edges,
           node_x_edge2 = sqrt(nodes*edges),
           log_node_x_edge = log(node_x_edge),
           log_time_per_iter = log(time_per_iter),
           log_time_taken = log(time_taken),
           log_edges = log(edges))
  
  

test <- uni_stats %>% select(file_name) %>%
  left_join(time_taken_df)
  
  time_taken__iter_mod <- lm(time_per_iter~edges, time_taken_df)
summary(time_taken__iter_mod)
#100kth of a second time increase per edge

box_cox_data_iter <- MASS::boxcox(time_taken__iter_mod, lambda = seq(-3, 3, 1/100))

tibble(lambda = box_cox_data_iter$x, log_likelihood = box_cox_data_iter$y) %>%
  arrange(-log_likelihood)
  
#actually time per iteration seems to be linear to the number of edges
Iteration_plot <-  time_taken_df %>%
    ggplot(aes(x = nodes, y = time_per_iter, colour = )) + geom_point()+
    scale_color_viridis_c()+
    labs(title = "Time per iteration for SETSe",
         y = "seconds per iteration",
         x = "number of edges in graph")
  
Iteration_plot   
  


#The time taken to convergence appears to be quadratic aka O(2) time. 
#This is very encouraging! Although it should be noted there is quite a lot of variation
#which is dependent on the topology of the network.. maybe related to elevation or tension? 
#this could be related to the communities that form

time_taken_df %>%
  ggplot(aes(x = edges, y = time_taken )) + geom_point()+
  labs(title ="Time to convergence is heteroscedastic for the facebook social networks",
       y = "minutes to convergence",
       x = "number of edges")
#The relationship is not exponential as can be demonstrated using a simple test on exponential vs quadratic toy data
time_taken_mod <- lm(time_taken~edges, time_taken_df)
time_taken_mod2 <- lm(log_time_taken~log_edges, time_taken_df)
summary(time_taken_mod)
summary(time_taken_mod2)


#bind_rows(time_taken_df %>% mutate(type2 = "bicomp"), time_taken_df_auto %>% mutate(type2 = "auto")) %>%
time_taken_df %>%
  select(file_name, edges, time_taken, time_per_iter) %>%
  pivot_longer(., cols = time_taken:time_per_iter, names_to = "type") %>%
  mutate(type = str_replace_all(type, "_", " ")) %>%
  ggplot(aes(x = edges, y = value)) + geom_point() + scale_color_viridis_c()+ 
  facet_wrap(~type, scales = "free_y") +
  labs(title = "Time complexity of SETSe on iteration and covnergence of Facebook 100 data",
       x = "number of edges",
       y = "seconds")
ggsave(file.path(FiguresFolder, "time_complexity.pdf"))

box_cox_data <-MASS::boxcox(time_taken_mod, lambda = seq(-3, 3, 1/100))

tibble(lambda = box_cox_data$x, log_likelihood = box_cox_data$y) %>%
  arrange(-log_likelihood)


file_paths <- list.files("/home/jonno/setse_1_data/facebook_embeddings/facebookbicomp", pattern = "facebook.e", full.names = T)

memory_df <- 1:length(file_paths) %>% map_df(~{
  

 file_number_to_open <-grep(paste0("\\.", .x, "$"), file_paths)

 file_name <- file_paths[file_number_to_open]

 print_data <- read_lines(file_name)

tibble( Mbytes = print_data[grep("Maximum resident set size", print_data)] %>% str_extract(., "[0-9]+") %>% as.numeric()/1000, 
       file_id = basename(file_name) %>% str_remove(., "facebook.e4053572.") %>% as.numeric())

}) 

memory_df <- memory_df %>% bind_cols(., uni_stats %>% arrange(nodes) %>% slice(1:nrow(memory_df)))


file_paths <- list.files("/home/jonno/setse_1_data/facebook_embeddings/facebookauto", pattern = "facebookauto.e", full.names = T)

memory_df_auto <- 1:length(file_paths) %>% map_df(~{
  

 file_number_to_open <-grep(paste0("\\.", .x, "$"), file_paths)

 file_name <- file_paths[file_number_to_open]

 print_data <- read_lines(file_name)

tibble( Mbytes = print_data[grep("Maximum resident set size", print_data)] %>% str_extract(., "[0-9]+") %>% as.numeric()/1000, 
       file_id = basename(file_name) %>% str_remove(., "facebookauto.e4053573.") %>% as.numeric())

}) 

memory_df_auto <- memory_df_auto %>% bind_cols(., uni_stats %>% arrange(nodes) %>% slice(1:nrow(memory_df_auto)))

bind_rows(memory_df %>% mutate(type = "bicomp"), memory_df_auto %>% mutate(type = "auto")) %>%
ggplot(aes(x = edges, y =Mbytes, colour = type)) + geom_point()+
  labs(title = "Memory requirements vs number of edges.",
      subtitle = "Memory requirements are broadly linear, although there is some heteroscedasticity")

memory_mod <- lm(Mbytes~edges, test)
#1.2kb per edge ram requirement

summary(memory_mod)
 MASS::boxcox(memory_mod, lambda = seq(-3, 3, 1/100))

```


##Plot individual uni

This chunk allow the plotting of individual uni's

```{r}

#c("Dartmouth6", "Wellesley22", "Wesleyan43", "Haverford76")
#c("Auburn71", "Pepperdine86", "Rice31", "Simmons81")
#c("Auburn71", "Mich67",  "Maine59", "BC17")
 
#The uni's to be plotted
target_unis <-   c("Auburn71", "Mich67",  "Maine59", "BC17")
#load files
comparison_df <-    target_unis %>%
    map_df(~{
      readRDS(file.path(PLwd, "facebook_embeddings/facebook",
                           paste0(.x, ".rds")))$node_detail
           })

#create text to put on each plot
dat_text <- comparison_df %>% 
  rename(file_name = uni) %>%
  group_by(file_name) %>%
  summarise(mean_tension = mean(mean_tension),
            abs_elevation = mean(abs(elevation))) %>%
  left_join(uni_stats) %>%
  select(uni = file_name, year_assort, mean_tension, abs_elevation, ) %>%
  mutate_at(., vars(-uni), signif, 2) %>%
  mutate(text_message = paste0("assort=", year_assort, " abs elevation=", abs_elevation, " tension=", mean_tension ))



#lower tension gives more compact shapes
#lower elevation indicates many nodes with few connections outside ther group
comparison_df %>%
  filter((year %% 1)==0,
         
         #     uni =="Auburn71",
         log(mean_tension)>-2.5
  ) %>%
  ggplot(aes(x = log(median_tension), y = elevation)) + 
  geom_point(aes(colour = factor(year)), alpha = 1) +
  facet_wrap(~uni) + 
  geom_text(
    data    = dat_text,
    mapping = aes(x = -Inf, y = -Inf, label = text_message),
    hjust   = -0.1,
    vjust   = -1
  ) +
  labs(title = "Node level Facebook embeddings of selected universities",
       x = "log mean tension",
       color = "Year")
ggsave(file.path(FiguresFolder,  "node_level_embeddings_of_universities.pdf"))   

```

##Classificiation

This chunk compares nearest neighbout classification using setse against neighbour classification using the original graph

The setse embeddings beats the network neighbour approach on average across all values of k, for all metrics.

Accuracy is always better as SETSe will choose the majority class whilst network neighbours won't.
Conversely this means that the network neighbours can have higher balanced accuracy.

2004 performs worse than 2005 for both as the datasets are more skewed in 2004.
In fact as facebook was so new in 2004 many people may not have signed up skewing the results somewhat.
I will use the results only for 2005.

for 2005 SETSe outperforms graph neighbour voting for all values of k, for balanced accuracy, accuracy and cohens kappa

```{r}

 fb_metrics <- metric_set(accuracy, kap, bal_accuracy, f_meas)
source(file = file.path(SubcodeFolder, "compare_embeddings_with_graph.R"))

class_perf <- list.files(file.path(PLwd, "facebook_classifier"), full.names = TRUE) %>%
  map_df(readRDS) %>%
  mutate(estimate	= ifelse(is.na(estimate	), 1, estimate),
         network_knn = ifelse(is.na(network_knn), 1, network_knn)) %>%
  mutate(diff = estimate-network_knn,
         better = (diff>0)*1,
         naive = network_type2/(network_type1+network_type2),
         better_naive = estimate>naive,
         metric = case_when(
           metric =="bal_accuracy"~"Balanced accuracy",
           metric =="f_meas" ~"F measure",
           metric =="accuracy" ~"Accuracy",
           TRUE ~"Cohen's kappa"
         ))  


relative_embeddings <-class_perf %>%
  filter(!knn_failed) %>%
  group_by(metric, active_period, k, model) %>%
  summarise(better = mean(better, na.rm = T),
            better_naive = mean(better_naive, na.rm  = T),
            na_val = sum(is.na(better)),
            student_1 = sum(student_1),
            student_2 = sum(student_2))


#create dummy data to get the colours out.
#Using the colour wheel is probably smoother but whatever
colour_data <- {tibble(id = LETTERS[1:6], value = 1:6) %>%
  ggplot(aes(x = id, y = value, fill = id)) + geom_col()} %>% ggplot_build(.)




relative_embeddings %>%
 # filter(active_period==2005) %>%
  ggplot(aes(x = k, y = better, colour = model))+
  facet_wrap(~metric, scales ="free_y") + 
  geom_line() +
  labs(title = "Predicting student type from year embeddings relative to graph neighbour voting",
       x = "number of nearest neighbours",
       y ="fraction of times embedding beats graph neighbour voting") + 
  scale_colour_manual(values = colour_data$data[[1]]$fill[-4])
ggsave(file.path(FiguresFolder,  "facebook_knn_vs_graph_vote.pdf"))   

```

##2 dimension projections

I realised I can project a network using two variables to get a two dimensional representation

for this I need to use edge normalisation to get the forces on the same scale. Or somesort of normalisation anyway

This chuunk can be held in reserve. Really it opens up a lot more questions and may be better spun out into a new paper. It starts looking more at machine learning on SETSe embeddings as opposed to introducing the embeddings method itself.

```{r}

graph_to_load <- list.files(file.path(PLwd,"facebook100/facebook100_igraph"), pattern = "Simmons81",
                            full.names = T)

  g <- readRDS(graph_to_load )  %>% #load file
    remove_small_components()  %>%
    facebook_year_clean() %>% prepare_SETSe_continuous(., node_names = "name", k = 1000, force_var = "year", 
                                                       sum_to_one = TRUE, 
                                                       distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)
  
  embeddings_data_year <- SETSe_bicomp(g, 
                                  tstep = 0.01,
                                  mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                                  tol = sum(abs(vertex_attr(g, "force")))/1000,
                                  verbose = TRUE, 
                                  sparse = TRUE, 
                                  sample = 100,
                                  static_limit = sum(abs(vertex_attr(g, "force")))) 
  
  
  g2 <- readRDS(graph_to_load )  %>% #load file
    remove_small_components()  %>%
    facebook_year_clean() %>% prepare_SETSe_binary(., node_names = "name", 
                                                   k = 1000, 
                                                   force_var = "student_faculty", 
                                                   positive_value =1,
                                                   sum_to_one = TRUE, 
                                                   distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)
  
  embeddings_data_student <- SETSe_bicomp(g2, 
                                       tstep = 0.01,
                                       mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                                       tol = sum(abs(vertex_attr(g, "force")))/1000,
                                       verbose = TRUE, 
                                       sparse = TRUE, 
                                       sample = 100,
                                       static_limit = sum(abs(vertex_attr(g, "force")))) 
  
    
  g3 <- readRDS(graph_to_load )  %>% #load file
    remove_small_components()  %>%
    facebook_year_clean() %>% prepare_SETSe_binary(., node_names = "name", 
                                                   k = 1000, 
                                                   force_var = "student_faculty", 
                                                   positive_value =2,
                                                   sum_to_one = TRUE, 
                                                   distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)
  
  embeddings_data_student2 <- SETSe_bicomp(g3, 
                                       tstep = 0.01,
                                       mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                                       tol = sum(abs(vertex_attr(g, "force")))/1000,
                                       verbose = TRUE, 
                                       sparse = TRUE, 
                                       sample = 100,
                                       static_limit = sum(abs(vertex_attr(g, "force")))) 
  
  
 test <-  left_join(embeddings_data_year$node_embeddings %>%
                      set_names(paste0("year_", names(.))) %>%
                      rename(node = year_node), 
                    embeddings_data_student$node_embeddings %>%
                      set_names(paste0("student_", names(.))) %>%
                      rename(node = student_node), by = "node") %>%
   left_join(embeddings_data_student$node_embeddings %>%
                      set_names(paste0("student1_", names(.))) %>%
                      rename(node = student1_node),by = "node") %>%
   left_join( as_data_frame(g, what = "vertices"), by = c("node" = "name"))
 
 


 test %>%
   mutate(year = case_when(
     (year %% 1)==0 ~year
   )) %>%
   filter(!is.na(year)) %>%
  
   
    ggplot(aes(x = year_elevation, y = student_elevation, colour = factor(year))) + geom_point(alpha = 0.5) +
   labs(title = "two dimensional plotting of network elevation",
        x = "graduation year embedding",
        y = "Student is type 1 embedding",
        colour = "value")
 
 
  test2 <-  left_join(embeddings_data_year$edge_embeddings %>%
                      set_names(paste0("year_", names(.))) %>%
                      rename(edge_name = year_edge_name), 
                    embeddings_data_student$edge_embeddings %>%
                      set_names(paste0("student_", names(.))) %>%
                      rename(edge_name = student_edge_name), by = "edge_name") 
  
  
  test2 %>%
   # mutate(year = case_when(
   #   (year %% 1)==0 ~year
   # )) %>%
   ggplot(aes(x = log(year_tension), y = log(student_tension))) + geom_point() +
   labs(title = "two dimensional plotting of network elevation",
        x = "graduation year embedding",
        y = "Student is type 1 embedding")

  
mod <-  glm(as.factor(year) ~year_elevation + student_elevation ,family=binomial(link='logit'), test) 
    summary(mod)


tibble(estimate = factor(predict(mod, type = "response")>0.5), truth = factor(test$student_faculty!=1)) %>%
  metrics(estimate = estimate, truth = truth)

##
##
## See if a model can be made with missing data 90% train 10% test
##
##


graph_to_load <- list.files(file.path(PLwd,"facebook100/facebook100_igraph"), pattern = "Simmons81",
                            full.names = T)

  g <- readRDS(graph_to_load )  %>% #load file
    remove_small_components()  %>%
    facebook_year_clean() 


test_df <- as_data_frame(g, what = "vertices")
set.seed(456)
embedding_fold <- vfold_cv(test_df, v = 10, strata = student_faculty)

#
# This goes in a loop
#


kfold_test_list <- 1:10 %>% map(~{
  
  #this is actually the vertices which should keep thier force atributes.
  #all others go to 0 and do not affect the analysis
  train_embeds_df <-test_df %>%
    mutate(student2 = ifelse(1:n() %in% embedding_fold$splits[[.x]]$in_id, student_faculty, -99),
           year2 = ifelse((1:n() %in% embedding_fold$splits[[.x]]$in_id) & (year%%1)==0, year, NA ),
           year2 = ifelse(is.na(year2), mean(year2, na.rm = TRUE), year2)
    )

  year_embeddings <- graph_from_data_frame(as_data_frame(g), directed = FALSE, vertices = train_embeds_df) %>% prepare_SETSe_continuous(., node_names = "name", k = 1000, force_var = "year2", 
                                                                                                                                        sum_to_one = TRUE, 
                                                                                                                                        distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000) %>%
    SETSe_bicomp(., 
                 tstep = 0.01,
                 mass = sum(abs(vertex_attr(., "force"))/2)/vcount(.),
                 tol = sum(abs(vertex_attr(., "force")))/1000,
                 verbose = TRUE, 
                 sparse = TRUE, 
                 sample = 100,
                 static_limit = sum(abs(vertex_attr(., "force")))) 
  
  
  student_embeddings <- graph_from_data_frame(as_data_frame(g), directed = FALSE, vertices = train_embeds_df) %>%
    prepare_SETSe_binary(., 
                         node_names = "name", 
                         k = 1000, force_var = "student2",
                         positive_value = 1, 
                         sum_to_one = TRUE,
                         distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)  %>%
    SETSe_bicomp(., 
                 tstep = 0.01,
                 mass = sum(abs(vertex_attr(., "force"))/2)/vcount(.),
                 tol = sum(abs(vertex_attr(., "force")))/1000,
                 verbose = TRUE, 
                 sparse = TRUE, 
                 sample = 100,
                 static_limit = sum(abs(vertex_attr(., "force")))) 
  
  #
  #
  #
  
  fold_df <- embedding_fold$splits$`1`$data[embedding_fold$splits$`1`$in_id,]
  embedded_combo <-  left_join(year_embeddings$node_embeddings %>%
                                 set_names(paste0("year_", names(.))) %>%
                                 rename(node = year_node), 
                               student_embeddings$node_embeddings %>%
                                 set_names(paste0("student_", names(.))) %>%
                                 rename(node = student_node), by = "node")
  
  train_df  <- embedded_combo  %>%
    left_join(., train_embeds_df, by = c("node" = "name")) %>%
    filter((1:n() %in% embedding_fold$splits[[.x]]$in_id))
  
  
  test_df  <- embedded_combo  %>%
    left_join(., train_embeds_df, by = c("node" = "name")) %>%
    filter(!(1:n() %in% embedding_fold$splits[[.x]]$in_id))
  
  mod <-  glm(as.factor(student2) ~year_elevation + student_elevation ,family=binomial(link='logit'), train_df) 
  #  summary(mod)
  
  student_perf <- tibble(estimate = factor((predict(mod,  newdata = test_df)>0.5)*1), 
                         truth = factor((test_df$student2==1)*1)) %>%
    fb_metrics(estimate = estimate, truth = truth) %>%
    mutate(fold = .x)
  
  
  modyear <-  ranger(factor(year2, levels = 2004:2009) ~year_elevation + student_elevation, train_df %>%
                       filter((year2 %% 1)==0)) 
  # summary(mod)
  
  year_perf <- tibble(estimate = predict(modyear, type = "response", data = test_df %>%
                                           filter((year2 %% 1)==0) )$predictions, 
                      truth = test_df %>%
                        filter((year2 %% 1)==0) %>% pull(year2) %>% factor(., levels = 2004:2009) ) %>%
    fb_metrics(estimate = estimate, truth = truth) %>%
    mutate(fold = .x)
  
  Out <- list(student_embeddings = student_embeddings, year_embeddings = year_embeddings,
              student_perf = student_perf, year_perf = year_perf)
  
  
  return(Out)
}
)


test <- kfold_test_list %>% transpose()

names(test)

test2 <- kfold_test_list %>%
  transpose() %>% {.[[3]]} %>%
  bind_rows()

test3 %>%
  ggplot(aes(x = .estimate)) +
  geom_density()+
  facet_wrap(.~.metric)

test3 <- kfold_test_list %>%
  transpose() %>% {.[[4]]} %>%
  bind_rows()

#Output should be a list of the following

#A dataframe of elevation embeddings with 4 columns node, year, student, fold.
#A dataframe of the folds tibble

#The actual cross validation can be done outside the embedding loop in the normal way
    
```


##facebook biconnecected test

```{r}
#find which nodes have the most static force

uni_pattern <- grep("Caltech", uni_files )

file_name <- file.path("/home/jonno/setse_1_data/facebook_embeddings/facebook_year",
                       paste0(.x, ".rds"))

g <- readRDS(uni_files[uni_pattern])  %>% #load file
        remove_small_components()  %>%
        facebook_year_clean() %>% prepare_SETSe_continuous(., k = 1000, force_var = "year", 
                                                           sum_to_one = FALSE, 
                                                           distance = 1) 

bicon_data <- biconnected.components(g)

caltech <- facebook_embeddings_data$node_detail %>%
  bind_rows() %>%
  filter(file_id == "Caltech36") 
#get the biconnected components list
caltech_bicon <-  biconnected.components(g) 

#find component sizes
component_size <- (caltech_bicon$components) %>% map_dbl(length)

#get names of vertices on largest bicomponent, this is the vast majority of nodes
caltech_test <- vertex_attr(g, "name",  index = caltech_bicon$components[[which.max(component_size)]]) %>%
  tibble(node = .,
         type = "main") %>%
  left_join(caltech,.) %>%
  mutate(type = ifelse(is.na(type), "minor", type))

#
# We can see from these two maps that the small bicomponents on the edge 
# are disproportionately high in static force compared to the rest of the network
#By solving using by connected component we may be able to reduce the amount of time spent calculating

caltech_test %>%
  ggplot(aes(x = log10(abs(static_force)) , y = elevation, colour = type )) + geom_point()
caltech_test %>%
  ggplot(aes(y = log10(abs(static_force)), x = type )) + geom_boxplot()

#
#
#

#as a simple test only the main biconnected component will be converged

#delete everything apart from the nodes on the main connected component
g2 <- delete.vertices(g, !(1:vcount(g) %in% caltech_bicon$components[[which.max(component_size)]])) %>%
  facebook_year_clean() %>% 
  prepare_SETSe_continuous(., k = 1000, force_var = "year", 
                           sum_to_one = FALSE, 
                           distance = 1000) 

start_time <- Sys.time()
embeddings_data <- auto_SETSe(g2, 
                              tstep = 0.01,
                              mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                              verbose = TRUE, 
                              sparse = T, 
                              sample = 100)

stop_time <- Sys.time()

start_time2 <- Sys.time()
embeddings_data <- SETSe_bicomp(g, 
                              tstep = 0.01,
                              mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                              tol = sum(abs(vertex_attr(g, "force")))/1000,
                              verbose = TRUE, 
                              sparse = T, 
                              sample = 100)

stop_time2 <- Sys.time()
#The convergence of the largest biconnected region only, is 3 times faster
time_diff <- stop_time2-start_time2 

embeddings_data$network_dynamics %>%
  filter(Iter!=0) %>%
  ggplot(aes(x = Iter, y = log10(static_force)))+ geom_line()

embeddings_data$node_embeddings %>%
  filter(Iter!=0) %>%
  ggplot(aes(x = elevation))+ geom_density()
      
```

#one vs all 

This section see' what happens when you do a one vs all test with all nodes. Do they produce sub class patterns? or does some other thing occur? Does this indicate nodal influence in a graph? If so that could lead to conflict outcome prediction

This raises the question of what happens if some of the nodes alligaiances are known... This leads on to the relgious debate networks.
```{r}


1:500 %>% walk(~{

  graph_ref <- .x
  target_file <- file.path(PLwd, "peel_influence", paste0("graph_ref_", graph_ref, ".rds") )
  
  if(!file.exists(target_file)){
    print(.x)

  g_out <- multi_quintet[[graph_ref]]
  
 Out  <- one_vs_all_SETSe(multi_quintet[[graph_ref]], k_options2)  
 
 saveRDS(Out, file = target_file)
  }
})

    

test <- 1:500 %>% 
  map_df(~{
      #I need to do this for all examples
      df <- read_rds(file.path(PLwd, "peel_influence2", paste0("graph_ref_", .x, ".rds") )) 
   df %>% 
      select(contains("clustering")) %>%
          map_dbl(~{
            
            adjustedRandIndex(df$sub_class, .x)
            
          }) %>% tibble(names = names(.), values = .) %>%
      mutate(graph = .x)

})


#The algorithm is substantially worse when using a completely unsupervised approach
#but why are all the other methods exactly the same each time?
test %>% 
  mutate(graphtype = case_when(
    graph<=100 ~"A",
    graph<=200 ~"B",
    graph<=300~"C",
    graph<=400~"D",
    TRUE ~"E"
  )) %>%
 # filter(names =="clustering_walktrap") %>%
  ggplot(aes(x = names, y = values, fill = names)) +
  geom_boxplot() +
  facet_wrap(~graphtype) +
  theme(axis.text.x = element_blank()) +
  labs(title = "ARI performance of clustering techniques when no group affiliation is given")

```
