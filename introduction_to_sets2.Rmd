---
title: "Untitled"
author: "Jonathan Bourne"
date: "09/03/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---


Possibly reviewers
Yong-Yeol Ahn
Assistant Professor
School of Informatics and Computing
Indiana University

Alessandro Flammini
Professor @ School of Informatics, Computing and Engineering at Indiana University, 

Any one in Emilio Ferrara's group at USC


Stephen G. Kobourov
Professor
Department of Computer Science
University of Arizona
kobourov@cs.arizona.edu

%postdoc of Kobourov
Felice DeLuca


repo on some interesting road safety network data
https://github.com/Moradii/stlnpp
The paper
https://doi.org/10.1016/j.spasta.2019.100400



#Set up
```{r Setup}

packages <- c("tidyverse", "igraph","readr","readxl", "broom", "stringr", "xtable", "rlang", "latex2exp", "yardstick", "minpack.lm", "ggraph", "patchwork", "rsample", "VGAM", "class", "mclust", "R.matlab", "ranger")

new.packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

select <- dplyr::select
arrange <- dplyr::arrange
map <- purrr::map
sapply(packages, library, character.only = TRUE)

library(rSETSe)

#Set up file system to read the correct folders this switches between aws and windows mode

#basewd <- "/home/jonno/Dropbox/Jonathan_Bourne_Phd_Folder"
LatexFolder <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Sets Paper 1" 
FiguresFolder <- file.path(LatexFolder, "Figures")
TablesFolder <- file.path(LatexFolder, "Tables")
MatricesFolder <- file.path(LatexFolder, "Matrices")
PLwd <- "/home/jonno/setse_1_data"
CodeFolder <- "/home/jonno/SETSe_assortativity_and_clusters"
SubcodeFolder <- file.path(CodeFolder, "sub_code")

#Load some other useful functions
list.files("/home/jonno/Useful_PhD__R_Functions", pattern = ".R", full.names = T) %>%
  walk(~source(.x))


```

#4 node 3 edge
provides the elevation position of the nodes in the opening example
```{r}

example_g <-graph_from_data_frame(tibble(from=c("A", "B", "B"), to = c("B", "C", "D")),
              directed = FALSE,
              vertices = tibble(node = c("A", "B", "C", "D"), node_name = node, force = c(2L,0L,-1L,-1L)))

example_df <- example_g %>%
  prepare_SETSe_continuous(., node_names = "node_name", k = 1000, force_var = "force") %>% 
  auto_SETSe()

example_df$node_embeddings %>% pull(elevation) %>% round(., 4)



```


#Peels quintet

This section re-creates the networks used in Peel et al

```{r}

source(file = file.path(SubcodeFolder, "prepare_peels_quintet_data.R"))

 (plot_list[[1]] | plot_list[[2]] | plot_list[[3]] )/
     (plot_list[[4]]| plot_list[[5]]) +
   plot_annotation(
  title = "The Peels quintet of assortativty identical graphs",
#  subtitle = 'The graphs can be separated using SETSe'
)
ggsave(file.path(FiguresFolder,  "Peels_quintet.pdf"))

rm(plot_list)
 

```

##Peel Strain

```{r}
#requires the "quintet" and "class_data_df" data frames
source(file = file.path(SubcodeFolder, "process_peel_strain.R"))
#the basic version used the original data

peel_strain_sum %>%
  filter(k==0) %>%
  group_by(graph_id, graph_type) %>%
  summarise_if(is.numeric, ~mean(abs(.x))) %>%
  ggplot(aes(x = mean_tension, y = elevation, colour = graph_type)) + geom_point(alpha = 0.8) +
  labs(title = "Separating Peel's quintet using elevation and strain",
       x = "Mean node average edge strain",
       y = "Mean node elevation of class A",
       colour = "graph type")
 # ggsave(file.path(FiguresFolder, "Seperating_Peels_quintet.pdf"))
#This demonstrates that the algorithm is topologically sensitive.
#Multiclass groups may be seperable using the 1 vs all method common in multiclass logitic regression, then seperating in a dimensional space equivalent to 2*(number of classes -1), that is beyond the scope of the paper.

```

##Peel knn


THIS SECTION IS NOT NECCESSARY AS THE graphs are linearly seperable.

This tests the accuracy of the classifier created by a knn and a multinomial logistic regression.
It shows that strain and elevation can accurately classify the graph types

accuract of the knn is about 83% and 84%. N.B the old data was higher (95% and 97%), but group B had a massive strain spread which was probably an error.%
```{r}
# #This needs to use some save files becuase it takes ages
# source(file = file.path(SubcodeFolder, "model_basic_peels.R"))
# 
# #Accuracy is between 95% and 96% for all values of k between 1 and ten
# nearest_k_results %>%
#   ggplot(aes(x = k, y = accuracy )) + 
#   geom_line() +
#   labs(title = "The mean accuracy for 100 repeats of 10 fold cross validation for knn classifier", 
#        y = "mean accuracy",
#        x ="nearest neighbours")
# #this file path is for casa presentation. delete after
# ggsave(file.path(FiguresFolder, "knn_accuracy.pdf"))


```

##Variable k
If the strength of relationship within and between groups varies then we findd that SETSe can distinguish between topologically identical networks that differ only in the strength of relationship.
```{r}


peel_strain_sum %>%
  ggplot(., aes( x= mean_tension_per_node, y = mean_abs_elevation, colour = as.factor(k))) + geom_point(alpha = 0.5) +
  facet_wrap(~graph_type, scales = "free_y") +
  labs(title = "The effect of different relationship strengths within and between groups on \ntension and elevation",
       colour = "relationship"
       )
ggsave(file.path(FiguresFolder, "strength_effect.pdf"))

peel_strain_sum %>%
  filter(
  ) %>%
  ggplot(., aes( x= mean_tension_per_node, y = mean_abs_elevation, colour = as.factor(k))) + geom_point(alpha = 0.5) +
  facet_wrap(~graph_type, scales = "free_y")



peel_strain_sum %>%
  ggplot(., aes( x= mean_tension_per_node, y = mean_abs_elevation, colour = as.factor(graph_type))) + geom_point(alpha = 0.5) +
  facet_wrap(~factor(k), scales = "free_y")

```


##The position of individual nodes 

This chunk shows that individual nodes naturally cluster by elevation with thier own sub groups sometimes the tension is also needed. I am not sure how to extract this without using a model though
```{r}
#There is possibly a problem with the node position code. The part that created detected_communities is producing 40k lines intead of 20k, this may or may not be a problem...

#....The 40k lines are becuase both groups a and be are checked.

source(file = file.path(SubcodeFolder, "node_position.R"))
 
#Use ARI to compare the quality of all clustering techniques
combos_df <- expand_grid(names = detected_communities %>% select(contains("clustering")) %>% names, graph_id = unique(detected_communities$graph_id))

test_ARI <-1:nrow(combos_df) %>%
  map_df(~{
    print(.x)
    temp <-detected_communities %>%
      filter(graph_id == combos_df$graph_id[.x])
    
    tibble(
      name = combos_df$names[.x],
      graph_id = combos_df$graph_id[.x],
      graph_class = unique(temp$graph_class),
      ARI = adjustedRandIndex(temp$sub_class, temp %>% pull(combos_df$names[.x]) ))
    
  })

#plot the results
#The ability to cluster graphs that have communities or groups that are not defined by their linking to each other greatly outstrips everything else

#plot and example of the true communities
detected_communities %>%
  mutate(sub_class = str_replace(sub_class, "_", " ")) %>%
  filter(graph_id %in% seq(100, 500, by = 100)) %>%
  ggplot(aes(x = tension_mean_range, y = elevation_range, colour = sub_class)) + geom_point() +
  facet_wrap(~graph_class) +
  labs(title = "The position of individual nodes in each graph type of Peel's Quintet",
       x = "node tension",
       y = "node elevation",
       color = "sub class") +
  theme(legend.position = c(0.9, 0.1), legend.justification = c(1, 0)) #positions the legend in the middle of the 6th panel
ggsave(file.path(FiguresFolder,  "peels_quintet_groups_in_setse_space.pdf"))   


 #plot showing how SETSe does the comparisons
 detected_communities %>%
    filter(graph_id %in% seq(100, 500, by = 100)) %>%
  ggplot(aes(x = tension_mean, y = elevation, colour = factor(clustering_kmeans))) + geom_point() +
  facet_wrap(~graph_class, scales = "free_y") +
   labs(title = "the position of each node in a graph coloured by kmeans groups",
        colour = "kmeans cluster")

              
        #when using knowlegde of the groups
        test_ARI %>%
          filter(!grepl("full", name)) %>%
          mutate(clustering = name %>% str_remove(., "clustering_") %>% str_replace(., "_", " "),
                 clustering = ifelse(clustering =="kmeans", "SETSe kmeans", clustering)) %>%
          ggplot(aes(x = clustering, y = ARI, fill = clustering)) + geom_boxplot() +
          facet_wrap(~graph_class) +
          labs(title = "Adjusted rand index for different clustering methods, for each graph type",
               x = "Clustering method") + 
          theme(
            axis.text.x=element_blank(),
            axis.ticks.x=element_blank())
    ggsave(file.path(FiguresFolder,  "peels_clustering_ari.pdf"))        
        
         #when not using knowledge of the groups
         test_ARI %>%
          filter(grepl("full", name))  %>%
          mutate(clustering = name %>% str_remove(., "clustering_") %>% str_replace(., "_", " "),
                 clustering = ifelse(clustering =="kmeans", "SETSe kmeans", clustering)) %>%
          ggplot(aes(x = clustering, y = ARI, fill = clustering)) + geom_boxplot() +
          facet_wrap(~graph_class) +
          labs(title = "Adjusted rand index for different clustering methods, for each graph type")

         #
 test <- test_ARI %>%
    group_by(name, graph_class) %>%
    summarise(mean = mean(ARI))
```


##n-dimensional strain

This is not used in the paper. IT is only kept so I don't forget

C relates to A and B like A and B relate to each other. C relates to itself, like B relates to itself

```{r}
C2B <- quintet %>%
  filter(class_1 != class_2) %>%
  mutate_at(., .vars = vars(sub_class_1:class_2), .funs = ~str_replace(., "A", "C"))

C2A <- quintet %>%
  filter(class_1 != class_2) %>%
  mutate_at(., .vars = vars(sub_class_1:class_2), .funs = ~str_replace(., "B", "C"))

C2C <-  quintet %>%
  filter(class_1 =="B", class_2 == "B") %>%
  mutate_at(., .vars = vars(sub_class_1:class_2), .funs = ~str_replace(., "B", "C"))

quintet_3d <- bind_rows(quintet, C2B, C2A, C2C)


class_data_df_3d <- expand.grid(class = c("A", "B", "C"), sub = 1:2) %>%
  as_tibble() %>%
  mutate(sub_class = paste(class, sub, sep = "_"),
         size = 10 #replace with a vector if group sizes uneevn
  )

set.seed(1235)
quintet_g_list <- LETTERS[1:5] %>%
  map(~create_assortivity_graph_from_subgroups(class_data_df_3d, quintet_3d %>% rename(edges = .x), 10) %>%
        set.edge.attribute(., "type", value = .x) %>%
        set.graph.attribute(., "type", value = .x)
      
  )

#Assortativity is not the same but who cares jsut use it anyway, there are two paires
1:5 %>% map_dbl(~{
  assortativity_nominal( quintet_g_list[[.x]], types = as.factor(vertex_attr( quintet_g_list[[.x]], "class")))
  
})



plot_list2 <- 1:5 %>%
  map(~{ggraph(quintet_g_list[[.x]]) +
      geom_edge_fan()+
      geom_node_point(aes(fill= class, shape = grepl("1", sub_class)), size=3) +
      scale_shape_manual(values=c(21, 24, 23)) +
      guides(fill = "none", shape = "none") +
      labs(title = paste("Type", LETTERS[.x]))})

#red is A green is B, blue is C
#triangle means subclass 1

 (plot_list2[[1]] | plot_list2[[2]] | plot_list2[[3]] )/
     (plot_list2[[4]]| plot_list2[[5]]) 



if(file.exists( file.path(PLwd, "setse_3d.rds"))){
 setse_3d<- readRDS( file.path(PLwd, "setse_3d.rds"))
} else {
source(file = file.path(SubcodeFolder, "process_peel_strain_3d.R"))
saveRDS(setse_3d, file.path(PLwd, "setse_3d.rds"))
}

  setse_3d %>%
  ggplot(aes(x = mean_tension_per_node, y = mean_euc_elev, colour = graph_type)) + geom_point()

peel_strain_sum <- list.files(file.path(PLwd, "peel_strain_files"), full.names= T) %>% 
  map_df(~{
    setse_complete <- readRDS(.x)

    Out  <-setse_complete$graphsummary %>%
      mutate(elevation = mean(sqrt(2*(setse_complete$node_embeddings$elevation^2))))
    return(Out)
  })

test <-peel_strain_sum %>% mutate(type = "binary") %>%
  filter(k==0) %>%
  bind_rows(setse_3d %>% mutate(type = "three classes"))


test %>%
  ggplot(aes(x = tension, y = elevation, colour = graph_type)) + geom_point()+
  facet_wrap(~type) + geom_point(alpha = 0.8) +
  labs(title = "Separating Peel's quintet using embedded height and tension for binary and 3 class cases",
       x = "Mean edge tension",
       y = "Mean node height of class A",
       colour = "graph type")
ggsave(file.path(FiguresFolder, "Seperating_Peels_quintet.pdf"))
```


#Facebook data

In this section I analyse the facebook 100 dataset. I got this from https://archive.org/details/oxford-2005-facebook-matrix

delete all nodes that graduated before 2004 and after 2009
keep only student types 1 and 2

normalise edge force to some fixed value e.g. 0.001 per edge


This is a paper on a facebook study that went a bit far and the data had to be removed.
worth commenting on probably
https://www-sciencedirect-com.libproxy.ucl.ac.uk/science/article/pii/S0378873308000385
```{r}

   g_df <- readRDS(uni_files[1]) %>% as_data_frame(., what = "vertices")

#convert the facebook .mat files to igraph format and save as rds
#this is commented out as it takes a long time and checking to see if the files are generated has not been done.
#source(file = file.path(SubcodeFolder, "convert_facebook_files.R"))

uni_files <- list.files("/home/jonno/setse_1_data/facebook100/facebook100_igraph", full.names = T)


#Create the biconnected data for the graphs with the time it takes to calculate.
source(file = file.path(SubcodeFolder, "facebook_biconnected.R"))

#This data frame shows that all uni's have a connected component of over 99%
#the smaller components can be disgarded
source(file = file.path(SubcodeFolder, "create_facebook_uni_stats.R"))

uni_stats %>%
  summarise(nodes = sum(max_component),
            student_type_1 = sum(student_type_1),
            student_type_2 =sum(student_type_2)) %>%
  mutate(perc_1 = student_type_1/nodes,
         perc_2 = student_type_2/nodes)

#I will use dorms over 0.5% of the total known university populations
#I will use years with over 1% of the population the rest will be reset to the mean of the students to not exert force

#In the one vs all use the height of the node when it is postive force and not when it isn't
#This is useful when you want to look at the distribution of the students.

#alternative use the euclidean distance of the mean of all positive force for the uni as a whole.
#This can be combined with the euclidean distance of the mean tension for a two dimensional projection

test2 <- left_join(uni_stats, biconnected_data) %>% as_tibble() %>%
  mutate(max_size_ratio = max_size/nodes) %>%
  select(uni_name:edges, total:max_size_ratio, density) %>%
  select(uni_name:nodes, max_size, max_size_ratio, everything()) %>%
  mutate(time_node = time/nodes)

#This creates year embeddings for all the facebook data. 
#It is commented out as the code is does not check if the file has been made
#source(file = file.path(SubcodeFolder, "facebook_embeddings_year.R"))
#source(file = file.path(SubcodeFolder, "process_facebook_embeddings.R"))


process_facebook_embeddings(file.path(PLwd, "facebook_embeddings" ,"facebook"), 
                            file.path(PLwd, "facebook_embeddings" , "processed_embeddings"))
process_facebook_embeddings(file.path(PLwd, "facebook_embeddings", "facebookauto"), 
                            file.path(PLwd, "facebook_embeddings", "processed_embeddings_auto"))

process_facebook_embeddings(file.path(PLwd, "facebook_embeddings", "facebook1000"), 
                            file.path(PLwd, "facebook_embeddings", "processed_embeddings_1000"))

```


##100 tension elevation
This plots the tension and elevation for all the uni's in the facebook
```{r}


  node_details_df <- readRDS(
    file.path("/home/jonno/setse_1_data/facebook_embeddings/processed_embeddings",
                                       "facebook_node_detail.rds" )) %>%
    mutate(abs_elevation = abs(elevation)) %>%
    group_by(file_name) %>%
    summarise_at(., vars(sum_tension:elevation , abs_elevation), mean, na.rm = T) %>%
    left_join(uni_stats, by = "file_name") %>%
  mutate(node_edge = nodes/edges)
  

test <- cor(node_details_df %>% select(sum_tension:euc_tension, abs_elevation, contains("assort")))# %>% as_tibble()
test <- cor(node_details_df %>% select( contains("assort")))# %>% as_tibble()

  
  #plot the facebook uni's by elevation and tension
#There is no relationship with size of uni which is good.
#there is a relationship between tension and elevation, which is less good I guess
#question is how does this relate to student housing 

#highlighting auburb main and michigen is a good idea to show how the assortativity of similar uni's is very separate
relate_figure <-  node_details_df %>%
  filter(mean_tension<1000) %>%
    ggplot(aes(y = abs_elevation, x = mean_tension, colour = year_assort, text = file_name)) + geom_point() + 
    scale_color_viridis_c() +
    labs(title = "The average elevation and tension embeddings of the facebook 100 dataset.",
         y = "mean absolute node elevation ",
         x = "mean node tension",
         color = "year\nassortativity")

relate_figure 
ggsave(file.path(FiguresFolder, "facebook_100_elev_tens.pdf"))

ggplotly(relate_figure)

node_details_df %>% ggplot(aes(x = student_assort)) + geom_density()

  #the year assortativity of the model can be predicted by the below linear model
  lm(year_assort ~ abs_elevation , node_details_df) %>% summary()
  lm(year_assort ~  mean_tension, node_details_df) %>% summary()
  
  assort_mod <- lm(year_assort ~ abs_elevation + mean_tension, node_details_df)
  summary(assort_mod)
  #It appears that a linear model predicting gender assortativity on the year embedding
  #has a stronger relationship than the straight corellation between gender and year assortativty.
  assort_mod <- lm(student_assort ~ abs_elevation  + mean_tension, node_details_df)
  summary(assort_mod)
  #the linear model predicting gender assortyativity from year is nothing
    lm(student_assort ~ year_assort, node_details_df) %>% summary
    
  
  node_details_df %>%
    ggplot(aes(y = abs_elevation, x = euc_tension, colour = gen_assort)) + geom_point() + 
    scale_color_viridis_c() 
  
    node_details_df %>%
    ggplot(aes(y = student_assort, x = year_assort, colour = gen_assort)) + geom_point() + 
    scale_color_viridis_c() 

  cor(node_details_df$mean_tension, node_details_df$abs_elevation)
  #euclidean tension has a substantially higher corellation
  cor(node_details_df$euc_tension, node_details_df$abs_elevation)
  
   node_details_df %>%
    ggplot(aes(x = mean_tension)) +
     geom_density()

   
   
```

##time memory complexity

This chunk shows that the iteration time complexity and the network memory demands are linear with the number of edges in the network.

```{r}

fb_network_dynamics <- readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/processed_embeddings", "facebook_network_dynamics.rds" ))

fb_network_dynamics_auto <- readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/processed_embeddings", "facebook_network_dynamics.rds" ))

#This is interesting but not very useful
  # fb_network_dynamics %>%
  #   left_join(uni_stats, by = c("file_name")) %>%
  # ggplot(aes(x = Iter, y = log10(static_force))) + geom_jitter()


#the time taken for an iteration is linear with the product of edges and nodes

  time_taken_df <- readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/processed_embeddings", "facebook_time_taken.rds" )) %>%
    mutate(time_taken = as.numeric(time_diff)) %>%
    left_join(., 
              fb_network_dynamics %>%
  bind_rows() %>%
    group_by(file_name) %>%
    summarise_all(last)
  ) %>%
    filter(nodes>=100) %>%
    mutate(time_per_iter = time_taken/Iter,
           node_x_edge = nodes*edges,
           node_x_edge2 = sqrt(nodes*edges),
           log_node_x_edge = log(node_x_edge),
           log_time_per_iter = log(time_per_iter),
           log_time_taken = log(time_taken),
           log_edges = log(edges))
  
time_taken_df_auto <- readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/processed_embeddings_auto", "facebook_time_taken.rds" )) %>%
    mutate(time_taken = as.numeric(time_diff)) %>%
    left_join(., 
              fb_network_dynamics %>%
  bind_rows() %>%
    group_by(file_name) %>%
    summarise_all(last)
  ) %>%
    filter(nodes>=100) %>%
    mutate(time_per_iter = time_taken/Iter,
           node_x_edge = nodes*edges,
           node_x_edge2 = sqrt(nodes*edges),
           log_node_x_edge = log(node_x_edge),
           log_time_per_iter = log(time_per_iter),
           log_time_taken = log(time_taken),
           log_edges = log(edges))
  
  

test <- uni_stats %>% select(file_name) %>%
  left_join(time_taken_df)
  
  time_taken__iter_mod <- lm(time_per_iter~edges, time_taken_df)
summary(time_taken__iter_mod)
#100kth of a second time increase per edge

box_cox_data_iter <- MASS::boxcox(time_taken__iter_mod, lambda = seq(-3, 3, 1/100))

tibble(lambda = box_cox_data_iter$x, log_likelihood = box_cox_data_iter$y) %>%
  arrange(-log_likelihood)
  
#actually time per iteration seems to be linear to the number of edges
  time_taken_df %>%
    ggplot(aes(x = edges, y = time_per_iter, colour = )) + geom_point()+
    scale_color_viridis_c()+
    labs(title = "SETSe time per iteration",
         y = "seconds per iteration",
         x = "number of edges in graph")
  
  
  


#The time taken to convergence appears to be quadratic aka O(2) time. 
#This is very encouraging! Although it should be noted there is quite a lot of variation
#which is dependent on the topology of the network.. maybe related to elevation or tension? 
#this could be related to the communities that form

time_taken_df %>%
  ggplot(aes(x = edges, y = time_taken )) + geom_point()+
  labs(title ="Time to convergence is heteroscedastic for the facebook social networks",
       y = "minutes to convergence",
       x = "number of edges")
#The relationship is not exponential as can be demonstrated using a simple test on exponential vs quadratic toy data
time_taken_mod <- lm(time_taken~edges, time_taken_df)
time_taken_mod2 <- lm(log_time_taken~log_edges, time_taken_df)
summary(time_taken_mod)
summary(time_taken_mod2)


#bind_rows(time_taken_df %>% mutate(type2 = "bicomp"), time_taken_df_auto %>% mutate(type2 = "auto")) %>%
time_taken_df %>%
  select(file_name, edges, time_taken, time_per_iter) %>%
  pivot_longer(., cols = time_taken:time_per_iter, names_to = "type") %>%
  mutate(type = str_replace_all(type, "_", " ")) %>%
  ggplot(aes(x = edges, y = value)) + geom_point() + scale_color_viridis_c()+ 
  facet_wrap(~type, scales = "free_y") +
  labs(title = "Time complexity of SETSe on iteration and covnergence of Facebook 100 data",
       x = "number of edges",
       y = "seconds")
ggsave(file.path(FiguresFolder, "time_complexity.pdf"))

box_cox_data <-MASS::boxcox(time_taken_mod, lambda = seq(-3, 3, 1/100))

tibble(lambda = box_cox_data$x, log_likelihood = box_cox_data$y) %>%
  arrange(-log_likelihood)


file_paths <- list.files("/home/jonno/setse_1_data/facebook_embeddings/facebookbicomp", pattern = "facebook.e", full.names = T)

memory_df <- 1:length(file_paths) %>% map_df(~{
  

 file_number_to_open <-grep(paste0("\\.", .x, "$"), file_paths)

 file_name <- file_paths[file_number_to_open]

 print_data <- read_lines(file_name)

tibble( Mbytes = print_data[grep("Maximum resident set size", print_data)] %>% str_extract(., "[0-9]+") %>% as.numeric()/1000, 
       file_id = basename(file_name) %>% str_remove(., "facebook.e4053572.") %>% as.numeric())

}) 

memory_df <- memory_df %>% bind_cols(., uni_stats %>% arrange(nodes) %>% slice(1:nrow(memory_df)))


file_paths <- list.files("/home/jonno/setse_1_data/facebook_embeddings/facebookauto", pattern = "facebookauto.e", full.names = T)

memory_df_auto <- 1:length(file_paths) %>% map_df(~{
  

 file_number_to_open <-grep(paste0("\\.", .x, "$"), file_paths)

 file_name <- file_paths[file_number_to_open]

 print_data <- read_lines(file_name)

tibble( Mbytes = print_data[grep("Maximum resident set size", print_data)] %>% str_extract(., "[0-9]+") %>% as.numeric()/1000, 
       file_id = basename(file_name) %>% str_remove(., "facebookauto.e4053573.") %>% as.numeric())

}) 

memory_df_auto <- memory_df_auto %>% bind_cols(., uni_stats %>% arrange(nodes) %>% slice(1:nrow(memory_df_auto)))

bind_rows(memory_df %>% mutate(type = "bicomp"), memory_df_auto %>% mutate(type = "auto")) %>%
ggplot(aes(x = edges, y =Mbytes, colour = type)) + geom_point()+
  labs(title = "Memory requirements vs number of edges.",
      subtitle = "Memory requirements are broadly linear, although there is some heteroscedasticity")

memory_mod <- lm(Mbytes~edges, test)
#1.2kb per edge ram requirement

summary(memory_mod)
 MASS::boxcox(memory_mod, lambda = seq(-3, 3, 1/100))

```



```{r}
uni_stats2 <-1:length(uni_files) %>%
  map_df(~{
    
    g <- readRDS(uni_files[.x]) %>%
      remove_small_components()
    #get the data on the uni's
    uni_name <- basename(uni_files[.x]) %>% str_remove(., ".rds") %>% str_remove(., "[0-9]+")
    data_id <- basename(uni_files[.x]) %>% str_remove(., ".rds") %>% str_remove(., "[ a-zA-Z]+") %>% 
      str_remove(., " ")
    
    print(uni_name)
    
    assortativity_nominal(g, types = as.factor(vertex_attr(g, "year")))
    
    g_df <- as_data_frame(g, what = "vertices") %>% tibble
    
    dorms_df <- facebook_fraction(g, "dorm")
    year_df <- facebook_fraction(g, "year")
    student_type <- vertex_attr(g, "student_faculty")
    
    tibble(file_number = .x, 
           file_name =  basename(uni_files[.x]) %>% str_remove(., ".rds"),
           uni_name = uni_name,
           data_id = data_id,
           nodes = vcount(g), edges = ecount(g),
           student_1 = sum(student_type==1)/vcount(g),
           student_2 = sum(student_type==2)/vcount(g),
           student_3 = sum(student_type==3)/vcount(g),
           student_4 = sum(student_type==4)/vcount(g),
           student_5 = sum(student_type==5)/vcount(g),
           student_6 = sum(student_type==6)/vcount(g),
           
    )
    
  })
```


##Plot individual uni

This chunk allow the plotting of individual uni's

```{r}
#penn test 
penn_test <-  readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/facebook","Penn94.rds"))
 penn_test$embeddings_data$network_dynamics %>%
  bind_rows() %>%
   # left_join(uni_stats, by = c("file_name")) %>%
  ggplot(aes(x = Iter, y = log10(static_force))) + geom_point()
 
 penn_test$node_detail %>%
   filter((year %%1)==0#,
   #       student_faculty %in% 1:2,
#          year %in% 2004:2005
) %>%
  # filter(log(euc_tension)>-1) %>%
   filter(year %in% c(2005)) %>%
   ggplot(aes(x = (mean_tension), y = elevation, color = factor(student_faculty))) + geom_point()+
   labs(title = "SETSe embeddings Penn State",
        color = "Year of enrollment")
 
ranger_df %>%
   ggplot(aes( x = (mean_tension2))) + geom_density()

#  theme(legend.position="bottom")

#means doesn't seem very useful here, probably becuase so much is going on
 fb_metrics <- metric_set(accuracy, kap, bal_accuracy, f_meas)
ranger_df <- penn_test$node_detail %>% filter(gender !=0,
                                         (year %% 1)==0,
                                         year %in% c(2005,2005),
                                         student_faculty %in% 1:2) %>%
  mutate(
        target = factor(student_faculty),
         euc_tension2 = (euc_tension-mean(euc_tension))/sd(euc_tension),
         mean_tension2 = (mean_tension-mean(mean_tension))/sd(mean_tension),
         elevation2 = (elevation-mean(elevation))/sd(elevation)) 


set.seed(54)
sample_vect <-  sample(1:nrow(ranger_df), size = round(nrow(ranger_df)*0.8), replace = TRUE)
train <- ranger_df %>% slice(sample_vect)
test <- ranger_df %>% slice((1:nrow(ranger_df))[!(1:nrow(ranger_df) %in% sample_vect)])


                
 model <- ranger(target~ mean_tension+ elevation + euc_tension + year, 
               data= train)

 voters <- 11
      mod <- class::knn.cv(train = train %>% select(mean_tension2, euc_tension2, elevation2), 
                    cl = factor(train$target), 
                   k = voters, prob = TRUE)
  
  #adjust for the fact that the node itself is included in the voting data

   
tibble(truth = test$target,
              estimate = predict(model, data = test)$predictions) %>%
  fb_metrics(truth = truth, estimate = estimate)

tibble(truth = train$target,
              estimate = mod) %>%
  fb_metrics(truth = truth, estimate = estimate)


 max(abs(penn_test$node_detail$static_force))

 penn_test2 <-  penn_test$node_detail %>%
    mutate(abs_static_force = abs(static_force),
           abs_force = abs(force.x),
           fract = abs_static_force/abs_force) 
  
  penn_test2 %>%
    ggplot(aes(x = log10(fract))) + geom_density()
 
  
   table(abs(penn_test$node_detail$static_force)<0.001)/nrow(penn_test$node_detail)

   
   table(round(penn_test$node_detail$force.x, 2))
   
  table(penn_test2$fract<1)
   
  #total static force error 
  sum(abs(penn_test$node_detail$static_force))/sum(abs(penn_test$node_detail$force.x))
  
  comparison_df <- bind_rows( 
    readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/HPC_embeddings","Caltech36.rds"))$node_detail,
    readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/HPC_embeddings","Simmons81.rds"))$node_detail)
  
   c("Dartmouth6", "Wellesley22", "Wesleyan43", "Haverford76")
  c("Auburn71", "Pepperdine86", "Rice31", "Simmons81")
  
  c("Auburn71", "Mich67",  "Maine59", "BC17")
  
  
target_unis <-   c("Auburn71", "Mich67",  "Maine59", "BC17")
comparison_df<-    target_unis %>%
    map_df(~{
      readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/facebook",
                           paste0(.x, ".rds")))$node_detail
           })
  
dat_text <- node_details_df %>%
  filter(file_name %in% target_unis) %>%
  select(uni = file_name, year_assort, mean_tension, euc_tension, abs_elevation, ) %>%
  mutate_at(., vars(-uni), signif, 2) %>%
  mutate(text_message = paste0("assort=", year_assort, " abs elevation=", abs_elevation, " tension=", mean_tension ))



#lower tension gives more compact shapes
#lower elevation indicates many nodes with few connections outside ther group
comparison_df %>%
  filter((year %% 1)==0,
         
         #     uni =="Auburn71",
         log(mean_tension)>-2.5
  ) %>%
  ggplot(aes(x = log(median_tension), y = elevation)) + 
  geom_point(aes(colour = factor(year)), alpha = 1) +
  facet_wrap(~uni) + 
  geom_text(
    data    = dat_text,
    mapping = aes(x = -Inf, y = -Inf, label = text_message),
    hjust   = -0.1,
    vjust   = -1
  ) +
  labs(title = "Node level Facebook embeddings of selected universities",
       x = "log mean tension",
       color = "Year")
ggsave(file.path(FiguresFolder,  "node_level_embeddings_of_universities.pdf"))   
  
comparison_df %>%
  filter((year %% 1)==0,
         log(mean_tension)>-2.5
  ) %>%
  ggplot(aes(x = log(median_tension), y = elevation)) + 
  geom_point(aes(colour = uni), alpha = 0.4) +
  facet_wrap(~factor(year)) 

#year centroid
    comparison_df %>%
    filter((year %% 1)==0) %>%
        group_by(year,
                 uni) %>%
        summarise(mean_elevation = mean(elevation),
                  mean_tension = mean(mean_tension)) %>%
    ggplot(aes(x = log(mean_tension), y = mean_elevation, colour =uni)) + 
    geom_point() +
    facet_wrap(~ factor(year)) 
  
    comparison_df %>%
    filter((year %% 1)==0,
           log(mean_tension)>-2.5) %>%
    ggplot(aes(x = (elevation), colour = factor(year))) + 
    geom_density() +
    facet_wrap(~uni)
    
    #The first year is never the highest tension
    #the highest tension is always the lowest years. However the first year is usualy one of the highest years.
    #The lowest tension years are those who are closest to the mean for the university.
    #This is counter intuitive as these years experience tension from all other yeas.
        comparison_df %>%
     filter((year %% 1)==0,
           log(mean_tension)>-2.5) %>%
    ggplot(aes(x = log10(mean_tension), colour = factor(year))) + 
    geom_density() +
    facet_wrap(~uni)
  
        
g %>%
  set_vertex_attr(., "year", value = as.factor(vertex_attr(g, "year")) %>% as.integer) %>%
  assortativity_nominal(., types = "year")
        
```

##Classificiation

This chunk compares nearest neighbout classificationusing setse against neighbour classification using the original graph


The setse embeddings beats the network neighbour approach on average across all values of k, for all metrics.

Accuracy is always better as SETSe will choose the majority class whilst network neighbours won't.
Conversely this means that the network neighbours can have higher balanced accuracy.

2004 performs worse than 2005 for both as the datasets are more skewed in 2004.
In fact as facebook was so new in 2004 many people may not have signed up skewing the results somewhat.
I will use the results only for 2005.

for 2005 SETSe outperforms graph neighbour voting for all values of k, for balanced accuracy, accuracy and cohens kappa

```{r}
 fb_metrics <- metric_set(accuracy, kap, bal_accuracy, f_meas)
source(file = file.path(SubcodeFolder, "compare_setse_with_graph.R"))

class_perf <- list.files(file.path(PLwd, "facebook_classifier"), full.names = TRUE) %>%
  map_df(readRDS) %>%
  mutate(estimate	= ifelse(is.na(estimate	), 1, estimate),
         network_knn = ifelse(is.na(network_knn), 1, network_knn)) %>%
  mutate(diff = estimate-network_knn,
         better = (diff>0)*1,
         naive = network_type2/(network_type1+network_type2),
         better_naive = estimate>naive)  

class_perf %>%
  group_by(file_name) %>%
  summarise_all(first) %>%# pull(naive) %>% mean
  ggplot(aes(x = naive)) + geom_density()

test <-class_perf %>%
#  filter(metric!= "accuracy") %>%
  group_by(metric, active_period, k) %>%
  summarise(better = mean(better, na.rm = T),
            better_naive = mean(better_naive, na.rm  = T),
            na_val = sum(is.na(better)),
            student_1 = sum(student_1),
            student_2 = sum(student_2))

test %>%
 # filter(active_period==2005) %>%
  ggplot(aes(x = k, y = better, colour = metric)) + geom_line() +
  labs(title = "Predicting student type from year embeddings relative to graph neighbour voting",
       x = "number of nearest neighbours in SETSe space k",
       y ="fraction of times SETSe beats graph neighbour voting")
ggsave(file.path(FiguresFolder,  "facebook_knn_vs_graph_vote.pdf"))   


test2 <- test%>%
  filter(active_period==2005)

class_perf2 <- class_perf %>%
  filter(active_period==2005) %>%
  group_by(metric, k) %>%
  summarise(estimate = mean(estimate),
            network_knn = mean(network_knn)) 

class_perf %>%
  ggplot(aes(x = as.factor(k), y = estimate, 
             #group = file_name,
             color = metric)) + geom_boxplot()+
  labs(title = "f1 score original")

prop.test(55, 100, alternative = "greater")

class_perf %>%
  filter(file_name =="Yale4") %>%
#  filter(active_period==2005) %>%
  mutate(diff = estimate-network_knn) %>%
ggplot(aes(x = k, y = diff, colour = metric)) + geom_point()+
  facet_wrap(~file_name)


class_perf_year <- list.files(file.path(PLwd, "facebook_classifier_year"), full.names = TRUE) %>%
  map_df(readRDS) %>%
  mutate(estimate	= ifelse(is.na(estimate	), 1, estimate),
         network_knn = ifelse(is.na(network_knn), 1, network_knn))%>%
  mutate(diff = estimate-network_knn,
         better = (diff>0)*1)  

test_year <-class_perf_year %>%
  group_by(metric, k) %>%
  summarise(better = mean(better, na.rm = T),
            mean_diff = mean(diff),
            na_val = sum(is.na(better)))

test_year %>%
  ggplot(aes(x = k, y = better, colour = metric)) + geom_line() +
  labs(title = "Predicting student type from year embeddings relative to graph neighbour voting",
       x = "number of nearest neighbours in SETSe space k",
       y ="Fraction of times SETSe beats graph neighbour voting")


class_perf_year2 <- class_perf_year %>%
  group_by(metric, k) %>%
  summarise(estimate = mean(estimate)) %>%
  filter(metric!="mcc")

class_perf_year2 %>%
  ggplot(aes(x = k, y = estimate, 
             color = metric)) + geom_line()

test <- class_perf %>%
  filter(metric =="f_meas") 


test <- bind_rows(
  class_perf_year %>%
  group_by(metric, k) %>%
  summarise(estimate = mean(estimate)) %>%
  filter(metric!="mcc") %>% mutate(type = "year") ,
class_perf %>%
  filter(active_period==2005) %>%
  group_by(metric, k) %>%
  summarise(estimate = mean(estimate)) %>% mutate(type = "type")
) %>% mutate(type = factor(type, levels = c("year", "type")))

test%>%
  ggplot(aes(x = k, y = estimate, 
             #group = file_name,
             color = metric)) + geom_line() +
  facet_grid(~type) +
  labs(title = "Performance of k nearest neighbour voting on year embeddings and the\nhidden embedding student type")
ggsave(file.path(FiguresFolder,  "facebook_knn_vs_graph_vote.pdf")) 
```

##2 dimension projections

I realised I can project a network using two variables to get a two dimensional representation

for this I need to use edge normalisation to get the forces on the same scale. Or somesort of normalisation anyway

This chuunk can be held in reserve. Really it opens up a lot more questions and may be better spun out into a new paper. It starts looking more at machine learning on SETSe embeddings as opposed to introducing the embeddings method itself.

```{r}

graph_to_load <- list.files(file.path(PLwd,"facebook100/facebook100_igraph"), pattern = "Simmons81",
                            full.names = T)

  g <- readRDS(graph_to_load )  %>% #load file
    remove_small_components()  %>%
    facebook_year_clean() %>% prepare_SETSe_continuous(., node_names = "name", k = 1000, force_var = "year", 
                                                       sum_to_one = TRUE, 
                                                       distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)
  
  embeddings_data_year <- SETSe_bicomp(g, 
                                  tstep = 0.01,
                                  mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                                  tol = sum(abs(vertex_attr(g, "force")))/1000,
                                  verbose = TRUE, 
                                  sparse = TRUE, 
                                  sample = 100,
                                  static_limit = sum(abs(vertex_attr(g, "force")))) 
  
  
  g2 <- readRDS(graph_to_load )  %>% #load file
    remove_small_components()  %>%
    facebook_year_clean() %>% prepare_SETSe_binary(., node_names = "name", 
                                                   k = 1000, 
                                                   force_var = "student_faculty", 
                                                   positive_value =1,
                                                   sum_to_one = TRUE, 
                                                   distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)
  
  embeddings_data_student <- SETSe_bicomp(g2, 
                                       tstep = 0.01,
                                       mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                                       tol = sum(abs(vertex_attr(g, "force")))/1000,
                                       verbose = TRUE, 
                                       sparse = TRUE, 
                                       sample = 100,
                                       static_limit = sum(abs(vertex_attr(g, "force")))) 
  
    
  g3 <- readRDS(graph_to_load )  %>% #load file
    remove_small_components()  %>%
    facebook_year_clean() %>% prepare_SETSe_binary(., node_names = "name", 
                                                   k = 1000, 
                                                   force_var = "student_faculty", 
                                                   positive_value =2,
                                                   sum_to_one = TRUE, 
                                                   distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)
  
  embeddings_data_student2 <- SETSe_bicomp(g3, 
                                       tstep = 0.01,
                                       mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                                       tol = sum(abs(vertex_attr(g, "force")))/1000,
                                       verbose = TRUE, 
                                       sparse = TRUE, 
                                       sample = 100,
                                       static_limit = sum(abs(vertex_attr(g, "force")))) 
  
  
 test <-  left_join(embeddings_data_year$node_embeddings %>%
                      set_names(paste0("year_", names(.))) %>%
                      rename(node = year_node), 
                    embeddings_data_student$node_embeddings %>%
                      set_names(paste0("student_", names(.))) %>%
                      rename(node = student_node), by = "node") %>%
   left_join(embeddings_data_student$node_embeddings %>%
                      set_names(paste0("student1_", names(.))) %>%
                      rename(node = student1_node),by = "node") %>%
   left_join( as_data_frame(g, what = "vertices"), by = c("node" = "name"))
 
 


 test %>%
   mutate(year = case_when(
     (year %% 1)==0 ~year
   )) %>%
   filter(!is.na(year)) %>%
  
   
    ggplot(aes(x = year_elevation, y = student_elevation, colour = factor(year))) + geom_point(alpha = 0.5) +
   labs(title = "two dimensional plotting of network elevation",
        x = "graduation year embedding",
        y = "Student is type 1 embedding",
        colour = "value")
 
 
  test2 <-  left_join(embeddings_data_year$edge_embeddings %>%
                      set_names(paste0("year_", names(.))) %>%
                      rename(edge_name = year_edge_name), 
                    embeddings_data_student$edge_embeddings %>%
                      set_names(paste0("student_", names(.))) %>%
                      rename(edge_name = student_edge_name), by = "edge_name") 
  
  
  test2 %>%
   # mutate(year = case_when(
   #   (year %% 1)==0 ~year
   # )) %>%
   ggplot(aes(x = log(year_tension), y = log(student_tension))) + geom_point() +
   labs(title = "two dimensional plotting of network elevation",
        x = "graduation year embedding",
        y = "Student is type 1 embedding")

  
mod <-  glm(as.factor(year) ~year_elevation + student_elevation ,family=binomial(link='logit'), test) 
    summary(mod)


tibble(estimate = factor(predict(mod, type = "response")>0.5), truth = factor(test$student_faculty!=1)) %>%
  metrics(estimate = estimate, truth = truth)

##
##
## See if a model can be made with missing data 90% train 10% test
##
##


graph_to_load <- list.files(file.path(PLwd,"facebook100/facebook100_igraph"), pattern = "Simmons81",
                            full.names = T)

  g <- readRDS(graph_to_load )  %>% #load file
    remove_small_components()  %>%
    facebook_year_clean() 


test_df <- as_data_frame(g, what = "vertices")
set.seed(456)
embedding_fold <- vfold_cv(test_df, v = 10, strata = student_faculty)

#
# This goes in a loop
#


kfold_test_list <- 1:10 %>% map(~{
  
  #this is actually the vertices which should keep thier force atributes.
  #all others go to 0 and do not affect the analysis
  train_embeds_df <-test_df %>%
    mutate(student2 = ifelse(1:n() %in% embedding_fold$splits[[.x]]$in_id, student_faculty, -99),
           year2 = ifelse((1:n() %in% embedding_fold$splits[[.x]]$in_id) & (year%%1)==0, year, NA ),
           year2 = ifelse(is.na(year2), mean(year2, na.rm = TRUE), year2)
    )

  year_embeddings <- graph_from_data_frame(as_data_frame(g), directed = FALSE, vertices = train_embeds_df) %>% prepare_SETSe_continuous(., node_names = "name", k = 1000, force_var = "year2", 
                                                                                                                                        sum_to_one = TRUE, 
                                                                                                                                        distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000) %>%
    SETSe_bicomp(., 
                 tstep = 0.01,
                 mass = sum(abs(vertex_attr(., "force"))/2)/vcount(.),
                 tol = sum(abs(vertex_attr(., "force")))/1000,
                 verbose = TRUE, 
                 sparse = TRUE, 
                 sample = 100,
                 static_limit = sum(abs(vertex_attr(., "force")))) 
  
  
  student_embeddings <- graph_from_data_frame(as_data_frame(g), directed = FALSE, vertices = train_embeds_df) %>%
    prepare_SETSe_binary(., 
                         node_names = "name", 
                         k = 1000, force_var = "student2",
                         positive_value = 1, 
                         sum_to_one = TRUE,
                         distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)  %>%
    SETSe_bicomp(., 
                 tstep = 0.01,
                 mass = sum(abs(vertex_attr(., "force"))/2)/vcount(.),
                 tol = sum(abs(vertex_attr(., "force")))/1000,
                 verbose = TRUE, 
                 sparse = TRUE, 
                 sample = 100,
                 static_limit = sum(abs(vertex_attr(., "force")))) 
  
  #
  #
  #
  
  fold_df <- embedding_fold$splits$`1`$data[embedding_fold$splits$`1`$in_id,]
  embedded_combo <-  left_join(year_embeddings$node_embeddings %>%
                                 set_names(paste0("year_", names(.))) %>%
                                 rename(node = year_node), 
                               student_embeddings$node_embeddings %>%
                                 set_names(paste0("student_", names(.))) %>%
                                 rename(node = student_node), by = "node")
  
  train_df  <- embedded_combo  %>%
    left_join(., train_embeds_df, by = c("node" = "name")) %>%
    filter((1:n() %in% embedding_fold$splits[[.x]]$in_id))
  
  
  test_df  <- embedded_combo  %>%
    left_join(., train_embeds_df, by = c("node" = "name")) %>%
    filter(!(1:n() %in% embedding_fold$splits[[.x]]$in_id))
  
  mod <-  glm(as.factor(student2) ~year_elevation + student_elevation ,family=binomial(link='logit'), train_df) 
  #  summary(mod)
  
  student_perf <- tibble(estimate = factor((predict(mod,  newdata = test_df)>0.5)*1), 
                         truth = factor((test_df$student2==1)*1)) %>%
    fb_metrics(estimate = estimate, truth = truth) %>%
    mutate(fold = .x)
  
  
  modyear <-  ranger(factor(year2, levels = 2004:2009) ~year_elevation + student_elevation, train_df %>%
                       filter((year2 %% 1)==0)) 
  # summary(mod)
  
  year_perf <- tibble(estimate = predict(modyear, type = "response", data = test_df %>%
                                           filter((year2 %% 1)==0) )$predictions, 
                      truth = test_df %>%
                        filter((year2 %% 1)==0) %>% pull(year2) %>% factor(., levels = 2004:2009) ) %>%
    fb_metrics(estimate = estimate, truth = truth) %>%
    mutate(fold = .x)
  
  Out <- list(student_embeddings = student_embeddings, year_embeddings = year_embeddings,
              student_perf = student_perf, year_perf = year_perf)
  
  
  return(Out)
}
)


test <- kfold_test_list %>% transpose()

names(test)

test2 <- kfold_test_list %>%
  transpose() %>% {.[[3]]} %>%
  bind_rows()

test3 %>%
  ggplot(aes(x = .estimate)) +
  geom_density()+
  facet_wrap(.~.metric)

test3 <- kfold_test_list %>%
  transpose() %>% {.[[4]]} %>%
  bind_rows()

#Output should be a list of the following

#A dataframe of elevation embeddings with 4 columns node, year, student, fold.
#A dataframe of the folds tibble

#The actual cross validation can be done outside the embedding loop in the normal way
    
```


##facebook biconnecected test

```{r}
#find which nodes have the most static force

uni_pattern <- grep("Caltech", uni_files )

file_name <- file.path("/home/jonno/setse_1_data/facebook_embeddings/facebook_year",
                       paste0(.x, ".rds"))

g <- readRDS(uni_files[uni_pattern])  %>% #load file
        remove_small_components()  %>%
        facebook_year_clean() %>% prepare_SETSe_continuous(., k = 1000, force_var = "year", 
                                                           sum_to_one = FALSE, 
                                                           distance = 1) 

bicon_data <- biconnected.components(g)

caltech <- facebook_embeddings_data$node_detail %>%
  bind_rows() %>%
  filter(file_id == "Caltech36") 
#get the biconnected components list
caltech_bicon <-  biconnected.components(g) 

#find component sizes
component_size <- (caltech_bicon$components) %>% map_dbl(length)

#get names of vertices on largest bicomponent, this is the vast majority of nodes
caltech_test <- vertex_attr(g, "name",  index = caltech_bicon$components[[which.max(component_size)]]) %>%
  tibble(node = .,
         type = "main") %>%
  left_join(caltech,.) %>%
  mutate(type = ifelse(is.na(type), "minor", type))

#
# We can see from these two maps that the small bicomponents on the edge 
# are disproportionately high in static force compared to the rest of the network
#By solving using by connected component we may be able to reduce the amount of time spent calculating

caltech_test %>%
  ggplot(aes(x = log10(abs(static_force)) , y = elevation, colour = type )) + geom_point()
caltech_test %>%
  ggplot(aes(y = log10(abs(static_force)), x = type )) + geom_boxplot()

#
#
#

#as a simple test only the main biconnected component will be converged

#delete everything apart from the nodes on the main connected component
g2 <- delete.vertices(g, !(1:vcount(g) %in% caltech_bicon$components[[which.max(component_size)]])) %>%
  facebook_year_clean() %>% 
  prepare_SETSe_continuous(., k = 1000, force_var = "year", 
                           sum_to_one = FALSE, 
                           distance = 1000) 

start_time <- Sys.time()
embeddings_data <- auto_SETSe(g2, 
                              tstep = 0.01,
                              mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                              verbose = TRUE, 
                              sparse = T, 
                              sample = 100)

stop_time <- Sys.time()

start_time2 <- Sys.time()
embeddings_data <- SETSe_bicomp(g, 
                              tstep = 0.01,
                              mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                              tol = sum(abs(vertex_attr(g, "force")))/1000,
                              verbose = TRUE, 
                              sparse = T, 
                              sample = 100)

stop_time2 <- Sys.time()
#The convergence of the largest biconnected region only, is 3 times faster
time_diff <- stop_time2-start_time2 

embeddings_data$network_dynamics %>%
  filter(Iter!=0) %>%
  ggplot(aes(x = Iter, y = log10(static_force)))+ geom_line()

embeddings_data$node_embeddings %>%
  filter(Iter!=0) %>%
  ggplot(aes(x = elevation))+ geom_density()
      
```

#one vs all 

This section see' what happens when you do a one vs all test with all nodes. Do they produce sub class patterns? or does some other thing occur? Does this indicate nodal influence in a graph? If so that could lead to conflict outcome prediction

This raises the question of what happens if some of the nodes alligaiances are known... This leads on to the relgious debate networks.
```{r}


1:500 %>% walk(~{

  graph_ref <- .x
  target_file <- file.path(PLwd, "peel_influence", paste0("graph_ref_", graph_ref, ".rds") )
  
  if(!file.exists(target_file)){
    print(.x)

  g_out <- multi_quintet[[graph_ref]]
  
 Out  <- one_vs_all_SETSe(multi_quintet[[graph_ref]], k_options2)  
 
 saveRDS(Out, file = target_file)
  }
})

    

test <- 1:500 %>% 
  map_df(~{
      #I need to do this for all examples
      df <- read_rds(file.path(PLwd, "peel_influence2", paste0("graph_ref_", .x, ".rds") )) 
   df %>% 
      select(contains("clustering")) %>%
          map_dbl(~{
            
            adjustedRandIndex(df$sub_class, .x)
            
          }) %>% tibble(names = names(.), values = .) %>%
      mutate(graph = .x)

})


#The algorithm is substantially worse when using a completely unsupervised approach
#but why are all the other methods exactly the same each time?
test %>% 
  mutate(graphtype = case_when(
    graph<=100 ~"A",
    graph<=200 ~"B",
    graph<=300~"C",
    graph<=400~"D",
    TRUE ~"E"
  )) %>%
 # filter(names =="clustering_walktrap") %>%
  ggplot(aes(x = names, y = values, fill = names)) +
  geom_boxplot() +
  facet_wrap(~graphtype) +
  theme(axis.text.x = element_blank()) +
  labs(title = "ARI performance of clustering techniques when no group affiliation is given")

```
