---
title: "Untitled"
author: "Jonathan Bourne"
date: "09/03/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---


Possibly reviewers
Yong-Yeol Ahn
Assistant Professor
School of Informatics and Computing
Indiana University

Alessandro Flammini
Professor @ School of Informatics, Computing and Engineering at Indiana University, 

Any one in Emilio Ferrara's group at USC


Stephen G. Kobourov
Professor
Department of Computer Science
University of Arizona

%postdoc of Kobourov
Felice DeLuca

```{r}
library(e1071)
```

#Set up
```{r Setup}

packages <- c("tidyverse", "igraph","readr","readxl", "broom", "stringr", "xtable", "rlang", "latex2exp", "yardstick", "minpack.lm", "ggraph", "patchwork", "rsample", "VGAM", "class", "mclust", "R.matlab", "ranger")

new.packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

select <- dplyr::select
arrange <- dplyr::arrange
map <- purrr::map
sapply(packages, library, character.only = TRUE)

library(NetworkSpringEmbedding)

#Set up file system to read the correct folders this switches between aws and windows mode

#basewd <- "/home/jonno/Dropbox/Jonathan_Bourne_Phd_Folder"
LatexFolder <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Sets Paper 1" 
FiguresFolder <- file.path(LatexFolder, "Figures")
TablesFolder <- file.path(LatexFolder, "Tables")
MatricesFolder <- file.path(LatexFolder, "Matrices")
PLwd <- "/home/jonno/setse_1_data"
CodeFolder <- "/home/jonno/SETSe_assortativity_and_clusters"
SubcodeFolder <- file.path(CodeFolder, "sub_code")

#Load some other useful functions
list.files("/home/jonno/Useful_PhD__R_Functions", pattern = ".R", full.names = T) %>%
  walk(~source(.x))


```

#4 node 3 edge
provides the elevation position of the nodes in the opening example
```{r}

example_g <-graph_from_data_frame(tibble(from=c("A", "B", "B"), to = c("B", "C", "D")),
              directed = FALSE,
              vertices = tibble(node = c("A", "B", "C", "D"), node_name = node, force = c(2L,0L,-1L,-1L)))

example_df <- example_g %>%
  prepare_SETSe_continuous(., node_names = "node_name", k = 1000, force_var = "force") %>% 
  auto_SETSe()

example_df$node_embeddings %>% pull(elevation) %>% round(., 4)



```


#Peels quintet

This section re-creates the networks used in Peel et al

```{r}

source(file = file.path(SubcodeFolder, "prepare_peels_quintet_data.R"))

 (plot_list[[1]] | plot_list[[2]] | plot_list[[3]] )/
     (plot_list[[4]]| plot_list[[5]]) +
   plot_annotation(
  title = "The Peels quintet of assortativty identical graphs",
#  subtitle = 'The graphs can be separated using SETSe'
)
ggsave(file.path(FiguresFolder,  "Peels_quintet.pdf"))

rm(plot_list)
 

```
##Peel Strain



```{r}
#requires the "quintet" and "class_data_df" data frames
source(file = file.path(SubcodeFolder, "process_peel_strain.R"))
#the basic version used the original data

peel_strain_sum %>%
  filter(k==0) %>%
  group_by(graph_id, graph_type) %>%
  summarise_if(is.numeric, ~mean(abs(.x))) %>%
  ggplot(aes(x = mean_tension, y = elevation, colour = graph_type)) + geom_point(alpha = 0.8) +
  labs(title = "Separating Peel's quintet using elevation and strain",
       x = "Mean node average edge strain",
       y = "Mean node elevation of class A",
       colour = "graph type")
 # ggsave(file.path(FiguresFolder, "Seperating_Peels_quintet.pdf"))
#This demonstrates that the algorithm is topologically sensitive.
#Multiclass groups may be seperable using the 1 vs all method common in multiclass logitic regression, then seperating in a dimensional space equivalent to 2*(number of classes -1), that is beyond the scope of the paper.

```

##Peel knn


THIS SECTION IS NOT NECCESSARY AS THE graphs are linearly seperable.

This tests the accuracy of the classifier created by a knn and a multinomial logistic regression.
It shows that strain and elevation can accurately classify the graph types

accuract of the knn is about 83% and 84%. N.B the old data was higher (95% and 97%), but group B had a massive strain spread which was probably an error.%
```{r}
# #This needs to use some save files becuase it takes ages
# source(file = file.path(SubcodeFolder, "model_basic_peels.R"))
# 
# #Accuracy is between 95% and 96% for all values of k between 1 and ten
# nearest_k_results %>%
#   ggplot(aes(x = k, y = accuracy )) + 
#   geom_line() +
#   labs(title = "The mean accuracy for 100 repeats of 10 fold cross validation for knn classifier", 
#        y = "mean accuracy",
#        x ="nearest neighbours")
# #this file path is for casa presentation. delete after
# ggsave(file.path(FiguresFolder, "knn_accuracy.pdf"))


```

##Variable k
If the strength of relationship within and between groups varies then we findd that SETSe can distinguish between topologically identical networks that differ only in the strength of relationship.
```{r}


peel_strain_sum %>%
  ggplot(., aes( x= mean_tension_per_node, y = mean_abs_elevation, colour = as.factor(k))) + geom_point(alpha = 0.5) +
  facet_wrap(~graph_type, scales = "free_y") +
  labs(title = "The effect of different relationship strengths within and between groups on \ntension and elevation",
       colour = "relationship"
       )
ggsave(file.path(FiguresFolder, "strength_effect.pdf"))

peel_strain_sum %>%
  filter(
  ) %>%
  ggplot(., aes( x= mean_tension_per_node, y = mean_abs_elevation, colour = as.factor(k))) + geom_point(alpha = 0.5) +
  facet_wrap(~graph_type, scales = "free_y")



peel_strain_sum %>%
  ggplot(., aes( x= mean_tension_per_node, y = mean_abs_elevation, colour = as.factor(graph_type))) + geom_point(alpha = 0.5) +
  facet_wrap(~factor(k), scales = "free_y")

```


#n-dimensional strain

C relates to A and B like A and B relate to each other. C relates to itself, like B relates to itself

```{r}
C2B <- quintet %>%
  filter(class_1 != class_2) %>%
  mutate_at(., .vars = vars(sub_class_1:class_2), .funs = ~str_replace(., "A", "C"))

C2A <- quintet %>%
  filter(class_1 != class_2) %>%
  mutate_at(., .vars = vars(sub_class_1:class_2), .funs = ~str_replace(., "B", "C"))

C2C <-  quintet %>%
  filter(class_1 =="B", class_2 == "B") %>%
  mutate_at(., .vars = vars(sub_class_1:class_2), .funs = ~str_replace(., "B", "C"))

quintet_3d <- bind_rows(quintet, C2B, C2A, C2C)


class_data_df_3d <- expand.grid(class = c("A", "B", "C"), sub = 1:2) %>%
  as_tibble() %>%
  mutate(sub_class = paste(class, sub, sep = "_"),
         size = 10 #replace with a vector if group sizes uneevn
  )

set.seed(1235)
quintet_g_list <- LETTERS[1:5] %>%
  map(~create_assortivity_graph_from_subgroups(class_data_df_3d, quintet_3d %>% rename(edges = .x), 10) %>%
        set.edge.attribute(., "type", value = .x) %>%
        set.graph.attribute(., "type", value = .x)
      
  )

#Assortativity is not the same but who cares jsut use it anyway, there are two paires
1:5 %>% map_dbl(~{
  assortativity_nominal( quintet_g_list[[.x]], types = as.factor(vertex_attr( quintet_g_list[[.x]], "class")))
  
})



plot_list2 <- 1:5 %>%
  map(~{ggraph(quintet_g_list[[.x]]) +
      geom_edge_fan()+
      geom_node_point(aes(fill= class, shape = grepl("1", sub_class)), size=3) +
      scale_shape_manual(values=c(21, 24, 23)) +
      guides(fill = "none", shape = "none") +
      labs(title = paste("Type", LETTERS[.x]))})

#red is A green is B, blue is C
#triangle means subclass 1

 (plot_list2[[1]] | plot_list2[[2]] | plot_list2[[3]] )/
     (plot_list2[[4]]| plot_list2[[5]]) 



if(file.exists( file.path(PLwd, "setse_3d.rds"))){
 setse_3d<- readRDS( file.path(PLwd, "setse_3d.rds"))
} else {
source(file = file.path(SubcodeFolder, "process_peel_strain_3d.R"))
saveRDS(setse_3d, file.path(PLwd, "setse_3d.rds"))
}

  setse_3d %>%
  ggplot(aes(x = mean_tension_per_node, y = mean_euc_elev, colour = graph_type)) + geom_point()

peel_strain_sum <- list.files(file.path(PLwd, "peel_strain_files"), full.names= T) %>% 
  map_df(~{
    setse_complete <- readRDS(.x)

    Out  <-setse_complete$graphsummary %>%
      mutate(elevation = mean(sqrt(2*(setse_complete$node_embeddings$elevation^2))))
    return(Out)
  })

test <-peel_strain_sum %>% mutate(type = "binary") %>%
  filter(k==0) %>%
  bind_rows(setse_3d %>% mutate(type = "three classes"))


test %>%
  ggplot(aes(x = tension, y = elevation, colour = graph_type)) + geom_point()+
  facet_wrap(~type) + geom_point(alpha = 0.8) +
  labs(title = "Separating Peel's quintet using embedded height and tension for binary and 3 class cases",
       x = "Mean edge tension",
       y = "Mean node height of class A",
       colour = "graph type")
ggsave(file.path(FiguresFolder, "Seperating_Peels_quintet.pdf"))
```


##check tension

I am not sure that the dimensions don't interact
```{r}

#two node network
matrix(c(0,0,0,
         1,3,4), nrow = 2, byrow = T)

diff <- c(1,3,4)

H3 <- sqrt(sum(diff^2))

H1 <- sqrt(sum(diff[c(1,2)]^2))
H2 <- sqrt(sum(diff[c(1,3)]^2))

H12 <- sqrt(sum(c(H1, H2)^2))
k <- 100
#end point of line
v1 <- c(1,0,0)
v2 <- c(0,3,0)
v3 <- c(0,0,4)
h <- v1+v2+v3


sqrt(1+3^2)
sqrt(1+16)

h_length <- sqrt(sum(h^2))
strain <- h_length-1
tension <- 100*strain


cosine_sim <- function(v, h){
  
vh <- sum(v*h)

sqrtv <- sqrt(sum(v^2))
sqrth <- sqrt(sum(h^2))
out <- vh/(sqrtv*sqrth)
#absv <- sum(abs(v))
#absh <- sum(abs(h))
#out <- vh/(absv*absh)

return(out)
}


cosine_sim2 <- function(v, h){
  #this is the same as cosine sim as the vectors are all one dimensional meaning the 
  #formula simplifies
v2 <- sum(v)

sqrtv <- sqrt(sum(v^2))
sqrth <- sqrt(sum(h^2))
out <- v2/(sqrth)
#absv <- sum(abs(v))
#absh <- sum(abs(h))
#out <- vh/(absv*absh)

return(out)
}


cosine_sim3 <- function(h, d){
  
  abs(h[d])/sqrt(sum(h^2))
  
}

cosine_sim4 <- function(h, d){
  #calculates the fraction of the hypotenuse contributed thus the fraction of tension
  h[d]^2/sum(h^2)
  
}


list(v1, v2,v3) %>%
  map_dbl(~cosine_sim2(v = .x, h = h)) %>%  {.^2}

v1_simp <-c(1, 0)
v2_simp <- c(0,3)
h_simp <- c(1,3)

list(v1_simp, v2_simp) %>%
  map_dbl(~cosine_sim2(v = .x, h = h_simp)) %>% {.*sqrt(10)}
  
1:3 %>%
  map_dbl(~cosine_sim3(h,.x)) %>% {.^2}

1:3 %>%
  map_dbl(~cosine_sim4(h,.x))


edge_ten2 <- function(a,d=1, k=1){
  
  k*(sqrt(a^2+ d^2)-d)
  
}

edge_ten <- function(a,d=1, k=1){
#calculate the edge tension of a n dimensional triangle  
  k*(sqrt(sum(c(a^2, d^2)))-d)
  
}


(c(edge_ten(3),edge_ten(4)))
edge_ten(c(3,4))


((c(edge_ten(3),edge_ten(4)))/edge_ten(c(3,4)))^2 %>% sum


edge_ten(c(3,4))


edge_dims <- c(1,3,40)  

  edge_dims %>%
  map_dbl(~cosine_sim2(.x,edge_dims)) %>% {.*edge_ten(edge_dims[-1])}

  c(1,3) %>%
  map_dbl(~cosine_sim2(.x,h[c(1,2)])) %>% {.^2*edge_ten(c(3))}


    c(1,4) %>%
  map_dbl(~cosine_sim2(.x,h[c(1,3)])) %>% {.^2*edge_ten(c(3))}

    
    
    c(  c(1,3) %>%
  map_dbl(~cosine_sim2(.x,h[c(1,2)])) %>% {.^2*edge_ten(c(3))},


    c(1,4) %>%
  map_dbl(~cosine_sim2(.x,h[c(1,3)])) %>% {.^2*edge_ten(c(3))}) %>% sum

a<- 3
d<- 10
k <- 1
     k*(sqrt(sum(c(a^2, d^2)))-1)
    
```


#Facebook data

In this section I analyse the facebook 100 dataset. I got this from https://archive.org/details/oxford-2005-facebook-matrix

delete all nodes that graduated before 2004 and after 2009
keep only student types 1 and 2

normalise edge force to some fixed value e.g. 0.001 per edge


This is a paper on a facebook study that went a bit far and the data had to be removed.
worth commenting on probably
https://www-sciencedirect-com.libproxy.ucl.ac.uk/science/article/pii/S0378873308000385
```{r}

   g_df <- readRDS(uni_files[1]) %>% as_data_frame(., what = "vertices")

#convert the facebook .mat files to igraph format and save as rds
#this is commented out as it takes a long time and checking to see if the files are generated has not been done.
#source(file = file.path(SubcodeFolder, "convert_facebook_files.R"))

uni_files <- list.files("/home/jonno/setse_1_data/facebook100/facebook100_igraph", full.names = T)


#Create the biconnected data for the graphs with the time it takes to calculate.
source(file = file.path(SubcodeFolder, "facebook_biconnected.R"))

#This data frame shows that all uni's have a connected component of over 99%
#the smaller components can be disgarded
source(file = file.path(SubcodeFolder, "create_facebook_uni_stats.R"))

#I will use dorms over 0.5% of the total known university populations
#I will use years with over 1% of the population the rest will be reset to the mean of the students to not exert force

#In the one vs all use the height of the node when it is postive force and not when it isn't
#This is useful when you want to look at the distribution of the students.

#alternative use the euclidean distance of the mean of all positive force for the uni as a whole.
#This can be combined with the euclidean distance of the mean tension for a two dimensional projection

test2 <- left_join(uni_stats, biconnected_data) %>% as_tibble() %>%
  mutate(max_size_ratio = max_size/nodes) %>%
  select(uni_name:edges, total:max_size_ratio, density) %>%
  select(uni_name:nodes, max_size, max_size_ratio, everything()) %>%
  mutate(time_node = time/nodes)

#This creates year embeddings for all the facebook data. 
#It is commented out as the code is does not check if the file has been made
#source(file = file.path(SubcodeFolder, "facebook_embeddings_year.R"))

source(file = file.path(SubcodeFolder, "process_facebook_embeddings.R"))

```

##100 tension elevation
This plots the tension and elevation for all the uni's in the facebook
```{r}



  node_details_df <- readRDS(
    file.path("/home/jonno/setse_1_data/facebook_embeddings/processed_embeddings",
                                       "facebook_node_detail.rds" )) %>%
    mutate(abs_elevation = abs(elevation)) %>%
    group_by(file_name) %>%
    summarise_at(., vars(sum_tension:elevation , abs_elevation), mean) %>%
    left_join(uni_stats, by = "file_name") %>%
  mutate(node_edge = nodes/edges)
  

test <- cor(node_details_df %>% select(sum_tension:euc_tension, abs_elevation, contains("assort")))# %>% as_tibble()
test <- cor(node_details_df %>% select( contains("assort")))# %>% as_tibble()

  
  #plot the facebook uni's by elevation and tension
#There is no relationship with size of uni which is good.
#there is a relationship between tension and elevation, which is less good I guess
#question is how does this relate to student housing 

#highlighting auburb main and michigen is a good idea to show how the assortativity of similar uni's is very separate
relate_figure <-  node_details_df %>%
    ggplot(aes(y = abs_elevation, x = mean_tension, colour = year_assort, text = file_name)) + geom_point() + 
    scale_color_viridis_c() +
    labs(title = "The average elevation and tension embeddings of the facebook 100 dataset.",
         y = "mean absolute node elevation ",
         x = "mean node tension",
         color = "year\nassortativity")

relate_figure 
ggsave(file.path(FiguresFolder, "facebook_100_elev_tens.pdf"))

ggplotly(relate_figure)

node_details_df %>% ggplot(aes(x = student_assort)) + geom_density()

  #the year assortativity of the model can be predicted by the below linear model
  lm(year_assort ~ abs_elevation , node_details_df) %>% summary()
  lm(year_assort ~  mean_tension, node_details_df) %>% summary()
  
  assort_mod <- lm(year_assort ~ abs_elevation + mean_tension, node_details_df)
  summary(assort_mod)
  #It appears that a linear model predicting gender assortativity on the year embedding
  #has a stronger relationship than the straight corellation between gender and year assortativty.
  assort_mod <- lm(student_assort ~ abs_elevation  + mean_tension, node_details_df)
  summary(assort_mod)
  #the linear model predicting gender assortyativity from year is nothing
    lm(student_assort ~ year_assort, node_details_df) %>% summary
    
  
  node_details_df %>%
    ggplot(aes(y = abs_elevation, x = euc_tension, colour = gen_assort)) + geom_point() + 
    scale_color_viridis_c() 
  
    node_details_df %>%
    ggplot(aes(y = student_assort, x = year_assort, colour = gen_assort)) + geom_point() + 
    scale_color_viridis_c() 

  cor(node_details_df$mean_tension, node_details_df$abs_elevation)
  #euclidean tension has a substantially higher corellation
  cor(node_details_df$euc_tension, node_details_df$abs_elevation)
  
   node_details_df %>%
    ggplot(aes(x = mean_tension)) +
     geom_density()

   
   
```

##time memory complexity

This chunk shows that the iteration time complexity and the network memory demands are linear with the number of edges in the network.

```{r}

fb_network_dynamics <- readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/processed_embeddings", "facebook_network_dynamics.rds" ))

#This is interesting but not very useful
  # fb_network_dynamics %>%
  #   left_join(uni_stats, by = c("file_name")) %>%
  # ggplot(aes(x = Iter, y = log10(static_force))) + geom_jitter()


#the time taken for an iteration is linear with the product of edges and nodes

  time_taken_df <- readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/processed_embeddings", "facebook_time_taken.rds" )) %>%
    mutate(time_taken = as.numeric(time_diff)) %>%
    left_join(., 
              fb_network_dynamics %>%
  bind_rows() %>%
    group_by(file_name) %>%
    summarise_all(last)
  ) %>%
    filter(nodes>=100) %>%
    mutate(time_per_iter = time_taken/Iter,
           node_x_edge = nodes*edges,
           node_x_edge2 = sqrt(nodes*edges),
           log_node_x_edge = log(node_x_edge),
           log_time_per_iter = log(time_per_iter),
           log_time_taken = log(time_taken),
           log_edges = log(edges))
  
  time_taken__iter_mod <- lm(time_per_iter~edges, time_taken_df)
summary(time_taken__iter_mod)
#100kth of a second time increase per edge

box_cox_data_iter <- MASS::boxcox(time_taken__iter_mod, lambda = seq(-3, 3, 1/100))

tibble(lambda = box_cox_data_iter$x, log_likelihood = box_cox_data_iter$y) %>%
  arrange(-log_likelihood)
  
#actually time per iteration seems to be linear to the number of edges
  time_taken_df %>%
    ggplot(aes(x = edges, y = time_per_iter, colour = )) + geom_point()+
    scale_color_viridis_c()+
    labs(title = "SETSe time per iteration",
         y = "seconds per iteration",
         x = "number of edges in graph")
  
  
  


#The time taken to convergence appears to be quadratic aka O(2) time. 
#This is very encouraging! Although it should be noted there is quite a lot of variation
#which is dependent on the topology of the network.. maybe related to elevation or tension? 
#this could be related to the communities that form

time_taken_df %>%
  ggplot(aes(x = edges, y = time_taken )) + geom_point()+
  labs(title ="Time to convergence is heteroscedastic for the facebook social networks",
       y = "minutes to convergence",
       x = "number of edges")
#The relationship is not exponential as can be demonstrated using a simple test on exponential vs quadratic toy data
time_taken_mod <- lm(time_taken~edges, time_taken_df)
time_taken_mod2 <- lm(log_time_taken~log_edges, time_taken_df)
summary(time_taken_mod)
summary(time_taken_mod2)


time_taken_df %>%
  select(edges, time_taken, time_per_iter) %>%
  pivot_longer(., cols = time_taken:time_per_iter, names_to = "type") %>%
  mutate(type = str_replace_all(type, "_", " ")) %>%
  ggplot(aes(x = edges, y = value)) + geom_point() +
  facet_wrap(~type, scales = "free_y") +
  labs(title = "Time complexity of SETSe on iteration and covnergence of Facebook 100 data",
       x = "number of edges",
       y = "seconds")
ggsave(file.path(FiguresFolder, "time_complexity.pdf"))

box_cox_data <-MASS::boxcox(time_taken_mod, lambda = seq(-3, 3, 1/100))

tibble(lambda = box_cox_data$x, log_likelihood = box_cox_data$y) %>%
  arrange(-log_likelihood)


file_paths <- list.files("/home/jonno/setse_1_data/facebook_embeddings/memory", full.names = T)

test <- 1:length(file_paths) %>% map_df(~{
  

 file_number_to_open <-grep(paste0("\\.", .x, "$"), file_paths)

 file_name <- file_paths[file_number_to_open]

 print_data <- read_lines(file_name)

tibble( Mbytes = print_data[grep("Maximum resident set size", print_data)] %>% str_extract(., "[0-9]+") %>% as.numeric()/1000, 
       file_id = basename(file_name) %>% str_remove(., "facebook.e3692260.") %>% as.numeric())

}) 

test <- test %>% bind_cols(., uni_stats %>% arrange(nodes) %>% slice(1:nrow(test)))

test %>%
ggplot(aes(x = edges, y =Mbytes)) + geom_point()+
  labs(title = "Memory requirements vs number of edges.",
      subtitle = "Memory requirements are broadly linear, although there is some heteroscedasticity")

memory_mod <- lm(Mbytes~edges, test)
#1.2kb per edge ram requirement

summary(memory_mod)
 MASS::boxcox(memory_mod, lambda = seq(-3, 3, 1/100))

```



```{r}
uni_stats2 <-1:length(uni_files) %>%
  map_df(~{
    
    g <- readRDS(uni_files[.x]) %>%
      remove_small_components()
    #get the data on the uni's
    uni_name <- basename(uni_files[.x]) %>% str_remove(., ".rds") %>% str_remove(., "[0-9]+")
    data_id <- basename(uni_files[.x]) %>% str_remove(., ".rds") %>% str_remove(., "[ a-zA-Z]+") %>% 
      str_remove(., " ")
    
    print(uni_name)
    
    assortativity_nominal(g, types = as.factor(vertex_attr(g, "year")))
    
    g_df <- as_data_frame(g, what = "vertices") %>% tibble
    
    dorms_df <- facebook_fraction(g, "dorm")
    year_df <- facebook_fraction(g, "year")
    student_type <- vertex_attr(g, "student_faculty")
    
    tibble(file_number = .x, 
           file_name =  basename(uni_files[.x]) %>% str_remove(., ".rds"),
           uni_name = uni_name,
           data_id = data_id,
           nodes = vcount(g), edges = ecount(g),
           student_1 = sum(student_type==1)/vcount(g),
           student_2 = sum(student_type==2)/vcount(g),
           student_3 = sum(student_type==3)/vcount(g),
           student_4 = sum(student_type==4)/vcount(g),
           student_5 = sum(student_type==5)/vcount(g),
           student_6 = sum(student_type==6)/vcount(g),
           
    )
    
  })
```


##Plot individual uni

This chunk allow the plotting of individual uni's

```{r}
#penn test 
penn_test <-  readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/HPC_embeddings","Penn94.rds"))
 penn_test$embeddings_data$network_dynamics %>%
  bind_rows() %>%
   # left_join(uni_stats, by = c("file_name")) %>%
  ggplot(aes(x = Iter, y = log10(static_force))) + geom_point()

 
 penn_test$node_detail %>%
   filter((year %%1)==0#,
   #       student_faculty %in% 1:2,
#          year %in% 2004:2005
) %>%
  # filter(log(euc_tension)>-1) %>%
   filter(year %in% c(2005)) %>%
   ggplot(aes(x = (mean_tension), y = elevation, color = factor(student_faculty))) + geom_point()+
   labs(title = "SETSe embeddings Penn State",
        color = "Year of enrollment")
 +
   facet_wrap(~factor(year))
 
ranger_df %>%
   ggplot(aes( x = (mean_tension2))) + geom_density()

#  theme(legend.position="bottom")

#means doesn't seem very useful here, probably becuase so much is going on
 fb_metrics <- metric_set(accuracy, kap, bal_accuracy, f_meas)
ranger_df <- penn_test$node_detail %>% filter(gender !=0,
                                         (year %% 1)==0,
                                         year %in% c(2005,2005),
                                         student_faculty %in% 1:2) %>%
  mutate(
        target = factor(student_faculty),
         euc_tension2 = (euc_tension-mean(euc_tension))/sd(euc_tension),
         mean_tension2 = (mean_tension-mean(mean_tension))/sd(mean_tension),
         elevation2 = (elevation-mean(elevation))/sd(elevation)) 


set.seed(54)
sample_vect <-  sample(1:nrow(ranger_df), size = round(nrow(ranger_df)*0.8), replace = TRUE)
train <- ranger_df %>% slice(sample_vect)
test <- ranger_df %>% slice((1:nrow(ranger_df))[!(1:nrow(ranger_df) %in% sample_vect)])


                
 model <- ranger(target~ mean_tension+ elevation + euc_tension + year, 
               data= train)

 voters <- 11
      mod <- class::knn.cv(train = train %>% select(mean_tension2, euc_tension2, elevation2), 
                    cl = factor(train$target), 
                   k = voters, prob = TRUE)
  
  #adjust for the fact that the node itself is included in the voting data

   
tibble(truth = test$target,
              estimate = predict(model, data = test)$predictions) %>%
  fb_metrics(truth = truth, estimate = estimate)

tibble(truth = train$target,
              estimate = mod) %>%
  fb_metrics(truth = truth, estimate = estimate)


 max(abs(penn_test$node_detail$static_force))

 penn_test2 <-  penn_test$node_detail %>%
    mutate(abs_static_force = abs(static_force),
           abs_force = abs(force.x),
           fract = abs_static_force/abs_force) 
  
  penn_test2 %>%
    ggplot(aes(x = log10(fract))) + geom_density()
 
  
   table(abs(penn_test$node_detail$static_force)<0.001)/nrow(penn_test$node_detail)

   
   table(round(penn_test$node_detail$force.x, 2))
   
  table(penn_test2$fract<1)
   
  #total static force error 
  sum(abs(penn_test$node_detail$static_force))/sum(abs(penn_test$node_detail$force.x))
  
  comparison_df <- bind_rows( 
    readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/HPC_embeddings","Caltech36.rds"))$node_detail,
    readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/HPC_embeddings","Simmons81.rds"))$node_detail)
  
   c("Dartmouth6", "Wellesley22", "Wesleyan43", "Haverford76")
  c("Auburn71", "Pepperdine86", "Rice31", "Simmons81")
  
  c("Auburn71", "Mich67",  "Maine59", "BC17")
  
  
target_unis <-   c("Auburn71", "Mich67",  "Maine59", "BC17")
comparison_df<-    target_unis %>%
    map_df(~{
      readRDS(file.path("/home/jonno/setse_1_data/facebook_embeddings/HPC_embeddings",
                           paste0(.x, ".rds")))$node_detail
           })
  
dat_text <- node_details_df %>%
  filter(file_name %in% target_unis) %>%
  select(uni = file_name, year_assort, mean_tension, euc_tension, abs_elevation) %>%
  mutate_at(., vars(-uni), signif, 2) %>%
  mutate(text_message = paste0("assort=", year_assort, " abs elevation=", abs_elevation, " tension=", mean_tension ))



#lower tension gives more compact shapes
#lower elevation indicates many nodes with few connections outside ther group
comparison_df %>%
  filter((year %% 1)==0,
         
         #     uni =="Auburn71",
         log(mean_tension)>-2.5
  ) %>%
  ggplot(aes(x = log(median_tension), y = elevation)) + 
  geom_point(aes(colour = factor(year)), alpha = 0.4) +
  facet_wrap(~uni) + 
  geom_text(
    data    = dat_text,
    mapping = aes(x = -Inf, y = -Inf, label = text_message),
    hjust   = -0.1,
    vjust   = -1
  ) +
  labs(title = "Node level facebook embeddings of selected universities",
       x = "log mean tension",
       color = "Year")
ggsave(file.path(FiguresFolder,  "node_level_embeddings_of_universities.pdf"))   
  
comparison_df %>%
  filter((year %% 1)==0,
         log(mean_tension)>-2.5
  ) %>%
  ggplot(aes(x = log(median_tension), y = elevation)) + 
  geom_point(aes(colour = uni), alpha = 0.4) +
  facet_wrap(~factor(year)) 

#year centroid
    comparison_df %>%
    filter((year %% 1)==0) %>%
        group_by(year,
                 uni) %>%
        summarise(mean_elevation = mean(elevation),
                  mean_tension = mean(mean_tension)) %>%
    ggplot(aes(x = log(mean_tension), y = mean_elevation, colour =uni)) + 
    geom_point() +
    facet_wrap(~ factor(year)) 
  
    comparison_df %>%
    filter((year %% 1)==0,
           log(mean_tension)>-2.5) %>%
    ggplot(aes(x = (elevation), colour = factor(year))) + 
    geom_density() +
    facet_wrap(~uni)
    
    #The first year is never the highest tension
    #the highest tension is always the lowest years. However the first year is usualy one of the highest years.
    #The lowest tension years are those who are closest to the mean for the university.
    #This is counter intuitive as these years experience tension from all other yeas.
        comparison_df %>%
     filter((year %% 1)==0,
           log(mean_tension)>-2.5) %>%
    ggplot(aes(x = log10(mean_tension), colour = factor(year))) + 
    geom_density() +
    facet_wrap(~uni)
  
        
g %>%
  set_vertex_attr(., "year", value = as.factor(vertex_attr(g, "year")) %>% as.integer) %>%
  assortativity_nominal(., types = "year")
        
```

##Classificiation

This chunk compares nearest neighbout classificationusing setse against neighbour classification using the original graph


The setse embeddings beats the network neighbour approach on average across all values of k, for all metrics.

Accuracy is always better as SETSe will choose the majority class whilst network neighbours won't.
Conversely this means that the network neighbours can have higher balanced accuracy.

2004 performs worse than 2005 for both as the datasets are more skewed in 2004.
In fact as facebook was so new in 2004 many people may not have signed up skewing the results somewhat.
I will use the results only for 2005.

for 2005 SETSe outperforms graph neighbour voting for all values of k, for balanced accuracy, accuracy and cohens kappa

```{r}
 fb_metrics <- metric_set(accuracy, kap, bal_accuracy, f_meas)
source(file = file.path(SubcodeFolder, "compare_setse_with_graph.R"))

class_perf <- list.files(file.path(PLwd, "facebook_classifier_student"), full.names = TRUE) %>%
  map_df(readRDS) %>%
  mutate(estimate	= ifelse(is.na(estimate	), 1, estimate),
         network_knn = ifelse(is.na(network_knn), 1, network_knn)) %>%
  mutate(diff = estimate-network_knn,
         better = (diff>0)*1,
         naive = network_type2/(network_type1+network_type2),
         better_naive = estimate>naive)  

class_perf %>%
  group_by(file_name) %>%
  summarise_all(first) %>%# pull(naive) %>% mean
  ggplot(aes(x = naive)) + geom_density()

test <-class_perf %>%
#  filter(metric!= "accuracy") %>%
  group_by(metric, active_period, k) %>%
  summarise(better = mean(better, na.rm = T),
            better_naive = mean(better_naive, na.rm  = T),
            na_val = sum(is.na(better)),
            student_1 = sum(student_1),
            student_2 = sum(student_2))

test %>%
 # filter(active_period==2005) %>%
  ggplot(aes(x = k, y = better, colour = metric)) + geom_line() +
  labs(title = "Predicting student type from year embeddings relative to graph neighbour voting",
       x = "number of nearest neighbours in SETSe space k",
       y ="Fraction of times SETSe beats graph neighbour voting")
ggsave(file.path(FiguresFolder,  "facebook_knn_vs_graph_vote.pdf"))   


test2 <- test%>%
  filter(active_period==2005)

class_perf2 <- class_perf %>%
  filter(active_period==2005) %>%
  group_by(metric, k) %>%
  summarise(estimate = mean(estimate),
            network_knn = mean(network_knn)) 

class_perf %>%
  ggplot(aes(x = as.factor(k), y = estimate, 
             #group = file_name,
             color = metric)) + geom_boxplot()+
  labs(title = "f1 score original")

prop.test(55, 100, alternative = "greater")

class_perf %>%
  filter(file_name =="Yale4") %>%
#  filter(active_period==2005) %>%
  mutate(diff = estimate-network_knn) %>%
ggplot(aes(x = k, y = diff, colour = metric)) + geom_point()+
  facet_wrap(~file_name)


class_perf_year <- list.files(file.path(PLwd, "facebook_classifier_year"), full.names = TRUE) %>%
  map_df(readRDS) %>%
  mutate(estimate	= ifelse(is.na(estimate	), 1, estimate),
         network_knn = ifelse(is.na(network_knn), 1, network_knn))%>%
  mutate(diff = estimate-network_knn,
         better = (diff>0)*1)  

test_year <-class_perf_year %>%
  group_by(metric, k) %>%
  summarise(better = mean(better, na.rm = T),
            mean_diff = mean(diff),
            na_val = sum(is.na(better)))

test_year %>%
  ggplot(aes(x = k, y = better, colour = metric)) + geom_line() +
  labs(title = "Predicting student type from year embeddings relative to graph neighbour voting",
       x = "number of nearest neighbours in SETSe space k",
       y ="Fraction of times SETSe beats graph neighbour voting")


class_perf_year2 <- class_perf_year %>%
  group_by(metric, k) %>%
  summarise(estimate = mean(estimate)) %>%
  filter(metric!="mcc")

class_perf_year2 %>%
  ggplot(aes(x = k, y = estimate, 
             color = metric)) + geom_line()

test <- class_perf %>%
  filter(metric =="f_meas") 


bind_rows(
  class_perf_year %>%
  group_by(metric, k) %>%
  summarise(estimate = mean(estimate)) %>%
  filter(metric!="mcc") %>% mutate(type = "year") ,
class_perf %>%
  filter(active_period==2005) %>%
  group_by(metric, k) %>%
  summarise(estimate = mean(estimate)) %>% mutate(type = "type")
) %>%
mutate(type = factor(type, levels = c("year", "type"))) %>%
  ggplot(aes(x = k, y = estimate, 
             #group = file_name,
             color = metric)) + geom_line() +
  facet_grid(~type) +
  labs(title = "Performance of k nearest neighbour voting on year embeddings and the\nhidden embedding student type")
ggsave(file.path(FiguresFolder,  "facebook_knn_vs_graph_vote.pdf")) 
```

##2 dimension projections

I realised I can project a network using two variables to get a two dimensional representation

for this I need to use edge normalisation to get the forces on the same scale. Or somesort of normalisation anyway

This chuunk can be held in reserve. Really it opens up a lot more questions and may be better spun out into a new paper. It starts looking more at machine learning on SETSe embeddings as opposed to introducing the embeddings method itself.

```{r}

graph_to_load <- list.files(file.path(PLwd,"facebook100/facebook100_igraph"), pattern = "Simmons81",
                            full.names = T)

  g <- readRDS(graph_to_load )  %>% #load file
    remove_small_components()  %>%
    facebook_year_clean() %>% prepare_SETSe_continuous(., node_names = "name", k = 1000, force_var = "year", 
                                                       sum_to_one = TRUE, 
                                                       distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)
  
  embeddings_data_year <- SETSe_bicomp(g, 
                                  tstep = 0.01,
                                  mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                                  tol = sum(abs(vertex_attr(g, "force")))/1000,
                                  verbose = TRUE, 
                                  sparse = TRUE, 
                                  sample = 100,
                                  static_limit = sum(abs(vertex_attr(g, "force")))) 
  
  
  g2 <- readRDS(graph_to_load )  %>% #load file
    remove_small_components()  %>%
    facebook_year_clean() %>% prepare_SETSe_binary(., node_names = "name", 
                                                   k = 1000, 
                                                   force_var = "student_faculty", 
                                                   positive_value =1,
                                                   sum_to_one = TRUE, 
                                                   distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)
  
  embeddings_data_student <- SETSe_bicomp(g2, 
                                       tstep = 0.01,
                                       mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                                       tol = sum(abs(vertex_attr(g, "force")))/1000,
                                       verbose = TRUE, 
                                       sparse = TRUE, 
                                       sample = 100,
                                       static_limit = sum(abs(vertex_attr(g, "force")))) 
  
    
  g3 <- readRDS(graph_to_load )  %>% #load file
    remove_small_components()  %>%
    facebook_year_clean() %>% prepare_SETSe_binary(., node_names = "name", 
                                                   k = 1000, 
                                                   force_var = "student_faculty", 
                                                   positive_value =2,
                                                   sum_to_one = TRUE, 
                                                   distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)
  
  embeddings_data_student2 <- SETSe_bicomp(g3, 
                                       tstep = 0.01,
                                       mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                                       tol = sum(abs(vertex_attr(g, "force")))/1000,
                                       verbose = TRUE, 
                                       sparse = TRUE, 
                                       sample = 100,
                                       static_limit = sum(abs(vertex_attr(g, "force")))) 
  
  
 test <-  left_join(embeddings_data_year$node_embeddings %>%
                      set_names(paste0("year_", names(.))) %>%
                      rename(node = year_node), 
                    embeddings_data_student$node_embeddings %>%
                      set_names(paste0("student_", names(.))) %>%
                      rename(node = student_node), by = "node") %>%
   left_join(embeddings_data_student$node_embeddings %>%
                      set_names(paste0("student1_", names(.))) %>%
                      rename(node = student1_node),by = "node") %>%
   left_join( as_data_frame(g, what = "vertices"), by = c("node" = "name"))
 
 


 test %>%
   mutate(year = case_when(
     (year %% 1)==0 ~year
   )) %>%
   filter(!is.na(year)) %>%
  
   
    ggplot(aes(x = year_elevation, y = student_elevation, colour = factor(year))) + geom_point(alpha = 0.5) +
   labs(title = "two dimensional plotting of network elevation",
        x = "graduation year embedding",
        y = "Student is type 1 embedding",
        colour = "value")
 
 
  test2 <-  left_join(embeddings_data_year$edge_embeddings %>%
                      set_names(paste0("year_", names(.))) %>%
                      rename(edge_name = year_edge_name), 
                    embeddings_data_student$edge_embeddings %>%
                      set_names(paste0("student_", names(.))) %>%
                      rename(edge_name = student_edge_name), by = "edge_name") 
  
  
  test2 %>%
   # mutate(year = case_when(
   #   (year %% 1)==0 ~year
   # )) %>%
   ggplot(aes(x = log(year_tension), y = log(student_tension))) + geom_point() +
   labs(title = "two dimensional plotting of network elevation",
        x = "graduation year embedding",
        y = "Student is type 1 embedding")

  
mod <-  glm(as.factor(year) ~year_elevation + student_elevation ,family=binomial(link='logit'), test) 
    summary(mod)


tibble(estimate = factor(predict(mod, type = "response")>0.5), truth = factor(test$student_faculty!=1)) %>%
  metrics(estimate = estimate, truth = truth)

##
##
## See if a model can be made with missing data 90% train 10% test
##
##


graph_to_load <- list.files(file.path(PLwd,"facebook100/facebook100_igraph"), pattern = "Simmons81",
                            full.names = T)

  g <- readRDS(graph_to_load )  %>% #load file
    remove_small_components()  %>%
    facebook_year_clean() 


test_df <- as_data_frame(g, what = "vertices")
set.seed(456)
embedding_fold <- vfold_cv(test_df, v = 10, strata = student_faculty)

#
# This goes in a loop
#


kfold_test_list <- 1:10 %>% map(~{
  
  #this is actually the vertices which should keep thier force atributes.
  #all others go to 0 and do not affect the analysis
  train_embeds_df <-test_df %>%
    mutate(student2 = ifelse(1:n() %in% embedding_fold$splits[[.x]]$in_id, student_faculty, -99),
           year2 = ifelse((1:n() %in% embedding_fold$splits[[.x]]$in_id) & (year%%1)==0, year, NA ),
           year2 = ifelse(is.na(year2), mean(year2, na.rm = TRUE), year2)
    )

  year_embeddings <- graph_from_data_frame(as_data_frame(g), directed = FALSE, vertices = train_embeds_df) %>% prepare_SETSe_continuous(., node_names = "name", k = 1000, force_var = "year2", 
                                                                                                                                        sum_to_one = TRUE, 
                                                                                                                                        distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000) %>%
    SETSe_bicomp(., 
                 tstep = 0.01,
                 mass = sum(abs(vertex_attr(., "force"))/2)/vcount(.),
                 tol = sum(abs(vertex_attr(., "force")))/1000,
                 verbose = TRUE, 
                 sparse = TRUE, 
                 sample = 100,
                 static_limit = sum(abs(vertex_attr(., "force")))) 
  
  
  student_embeddings <- graph_from_data_frame(as_data_frame(g), directed = FALSE, vertices = train_embeds_df) %>%
    prepare_SETSe_binary(., 
                         node_names = "name", 
                         k = 1000, force_var = "student2",
                         positive_value = 1, 
                         sum_to_one = TRUE,
                         distance = 100) %>%
    set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)  %>%
    SETSe_bicomp(., 
                 tstep = 0.01,
                 mass = sum(abs(vertex_attr(., "force"))/2)/vcount(.),
                 tol = sum(abs(vertex_attr(., "force")))/1000,
                 verbose = TRUE, 
                 sparse = TRUE, 
                 sample = 100,
                 static_limit = sum(abs(vertex_attr(., "force")))) 
  
  #
  #
  #
  
  fold_df <- embedding_fold$splits$`1`$data[embedding_fold$splits$`1`$in_id,]
  embedded_combo <-  left_join(year_embeddings$node_embeddings %>%
                                 set_names(paste0("year_", names(.))) %>%
                                 rename(node = year_node), 
                               student_embeddings$node_embeddings %>%
                                 set_names(paste0("student_", names(.))) %>%
                                 rename(node = student_node), by = "node")
  
  train_df  <- embedded_combo  %>%
    left_join(., train_embeds_df, by = c("node" = "name")) %>%
    filter((1:n() %in% embedding_fold$splits[[.x]]$in_id))
  
  
  test_df  <- embedded_combo  %>%
    left_join(., train_embeds_df, by = c("node" = "name")) %>%
    filter(!(1:n() %in% embedding_fold$splits[[.x]]$in_id))
  
  mod <-  glm(as.factor(student2) ~year_elevation + student_elevation ,family=binomial(link='logit'), train_df) 
  #  summary(mod)
  
  student_perf <- tibble(estimate = factor((predict(mod,  newdata = test_df)>0.5)*1), 
                         truth = factor((test_df$student2==1)*1)) %>%
    fb_metrics(estimate = estimate, truth = truth) %>%
    mutate(fold = .x)
  
  
  modyear <-  ranger(factor(year2, levels = 2004:2009) ~year_elevation + student_elevation, train_df %>%
                       filter((year2 %% 1)==0)) 
  # summary(mod)
  
  year_perf <- tibble(estimate = predict(modyear, type = "response", data = test_df %>%
                                           filter((year2 %% 1)==0) )$predictions, 
                      truth = test_df %>%
                        filter((year2 %% 1)==0) %>% pull(year2) %>% factor(., levels = 2004:2009) ) %>%
    fb_metrics(estimate = estimate, truth = truth) %>%
    mutate(fold = .x)
  
  Out <- list(student_embeddings = student_embeddings, year_embeddings = year_embeddings,
              student_perf = student_perf, year_perf = year_perf)
  
  
  return(Out)
}
)


test <- kfold_test_list %>% transpose()

names(test)

test2 <- kfold_test_list %>%
  transpose() %>% {.[[3]]} %>%
  bind_rows()

test3 %>%
  ggplot(aes(x = .estimate)) +
  geom_density()+
  facet_wrap(.~.metric)

test3 <- kfold_test_list %>%
  transpose() %>% {.[[4]]} %>%
  bind_rows()

#Output should be a list of the following

#A dataframe of elevation embeddings with 4 columns node, year, student, fold.
#A dataframe of the folds tibble

#The actual cross validation can be done outside the embedding loop in the normal way
    
```


##facebook biconnecected test

```{r}
#find which nodes have the most static force

uni_pattern <- grep("Caltech", uni_files )

file_name <- file.path("/home/jonno/setse_1_data/facebook_embeddings/facebook_year",
                       paste0(.x, ".rds"))

g <- readRDS(uni_files[uni_pattern])  %>% #load file
        remove_small_components()  %>%
        facebook_year_clean() %>% prepare_SETSe_continuous(., k = 1000, force_var = "year", 
                                                           sum_to_one = FALSE, 
                                                           distance = 1) 

bicon_data <- biconnected.components(g)

caltech <- facebook_embeddings_data$node_detail %>%
  bind_rows() %>%
  filter(file_id == "Caltech36") 
#get the biconnected components list
caltech_bicon <-  biconnected.components(g) 

#find component sizes
component_size <- (caltech_bicon$components) %>% map_dbl(length)

#get names of vertices on largest bicomponent, this is the vast majority of nodes
caltech_test <- vertex_attr(g, "name",  index = caltech_bicon$components[[which.max(component_size)]]) %>%
  tibble(node = .,
         type = "main") %>%
  left_join(caltech,.) %>%
  mutate(type = ifelse(is.na(type), "minor", type))

#
# We can see from these two maps that the small bicomponents on the edge 
# are disproportionately high in static force compared to the rest of the network
#By solving using by connected component we may be able to reduce the amount of time spent calculating

caltech_test %>%
  ggplot(aes(x = log10(abs(static_force)) , y = elevation, colour = type )) + geom_point()
caltech_test %>%
  ggplot(aes(y = log10(abs(static_force)), x = type )) + geom_boxplot()

#
#
#

#as a simple test only the main biconnected component will be converged

#delete everything apart from the nodes on the main connected component
g2 <- delete.vertices(g, !(1:vcount(g) %in% caltech_bicon$components[[which.max(component_size)]])) %>%
  facebook_year_clean() %>% 
  prepare_SETSe_continuous(., k = 1000, force_var = "year", 
                           sum_to_one = FALSE, 
                           distance = 1000) 

start_time <- Sys.time()
embeddings_data <- auto_SETSe(g2, 
                              tstep = 0.01,
                              mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                              verbose = TRUE, 
                              sparse = T, 
                              sample = 100)

stop_time <- Sys.time()

start_time2 <- Sys.time()
embeddings_data <- SETSe_bicomp(g, 
                              tstep = 0.01,
                              mass = sum(abs(vertex_attr(g, "force"))/2)/vcount(g),
                              tol = sum(abs(vertex_attr(g, "force")))/1000,
                              verbose = TRUE, 
                              sparse = T, 
                              sample = 100)

stop_time2 <- Sys.time()
#The convergence of the largest biconnected region only, is 3 times faster
time_diff <- stop_time2-start_time2 

embeddings_data$network_dynamics %>%
  filter(Iter!=0) %>%
  ggplot(aes(x = Iter, y = log10(static_force)))+ geom_line()

embeddings_data$node_embeddings %>%
  filter(Iter!=0) %>%
  ggplot(aes(x = elevation))+ geom_density()
      
```

#MNIST

This has a quick look at MNIST and sees what happens when you try and embed it using SETSe

```{r}

MNIST_df <- read_csv(file.path(PLwd, "MNIST", "mnist_train.csv"), col_names = FALSE)

1:6e4 %>% walk(~{
  
      #select single mnist
    mnist_nodes <- MNIST_df %>% slice(.x) %>%
      pivot_longer(cols = -X1) %>%
      mutate(
        index_val = 1:n(),
        rows = rep(1:28, times = 784/28 ),
        cols = rep(28:1, each = 784/28 ))
  
  file_path <- file.path(PLwd, "MNIST", "MNIST_embeddings",
                         paste0("number_", min(mnist_nodes$X1), "_id_", .x, ".rds"))
  
  if(file.exists(file_path)){print(paste("MNIST", .x, "completed, proceeding to next"))
    } else{
    

    
    
    #convert into a graph values totalling to 2000
    g_lattice <- make_lattice(c(28,28)) %>%
      set_vertex_attr(., "names", value = 1:784) %>%
      set_vertex_attr(., "value", value = mnist_nodes %>% pull(value)) %>%
      prepare_SETSe_continuous(., node_names = "names", 
                               k = 1000,
                               force_var = "value",
                               distance = 100,
                               sum_to_one = TRUE) %>%
      set_vertex_attr(., "force", value = vertex_attr(., "force")*1000)
    
    out <- auto_SETSe(g_lattice, force = "force", verbose = T,
                      mass = 2000/784,
                      tol = sum(abs(vertex_attr(g_lattice, "force")))/1000,
                      sparse = TRUE)
    
    out$node_details <- out$edge_embeddings %>% tibble() %>%
      separate(., col = edge_name, into = c("from","to"), sep = "-") %>%
      select(from, to, tension) %>%
      pivot_longer(cols = c(from, to), names_to = "node_type", values_to = "node") %>%
      select(tension, node) %>%
      group_by(node) %>%
      summarise(sum_tension = sum(tension),
                mean_tension = mean(tension),
                median_tension = median(tension),
                euc_tension = sqrt(sum(tension^2))) %>%
      left_join(out$node_embeddings %>% tibble(), by = "node") %>%
      left_join(as_data_frame(g_lattice, what = "vertices"), by = c("node"="name"))
    
    saveRDS(out, file_path)
    print(paste("MNIST", .x, "completed, proceeding to next"))
  }
  
})

test <- list.files(file.path(PLwd, "MNIST", "MNIST_embeddings"), full.names = T) %>%
  map_df(~{
    
    file_path <- .x
    
    details <- basename(file_path) %>% str_remove(., ".rds") %>% str_split(., "_", simplify = T)
    
    readRDS(.x)$node_details %>%
      summarise(mean_tension = mean(mean_tension),
                median_tension = mean(median_tension),
                euc_tension = mean(euc_tension),
                mean_elevation = mean(abs(elevation))) %>%
      mutate(number = details[2],
             id = details[4])
    
  })


test %>%
  ggplot(aes(x = euc_tension, y = mean_elevation, colour = factor(number))) + geom_point() +
  scale_color_brewer(palette ="Paired")

test <- as_data_frame(g_lattice, what = "vertices")

#The mnist edge list is calulcated by adding 1 to the index and the row, subtracting 1 from the index and the row, adding 28 to the index and 1 to the columns subtracting 28 from the index and the column. this will produce a large number of invalid cells. These cells can be removed by doing the following
#if the new row or columns doesn't exists remove it.

edge_list_base <- mnist_nodes %>%
  select(index_val:cols)

link_above <- edge_list_base %>%
  mutate(index_val = index_val-1,
         rows = rows-1)

link_below <- edge_list_base %>%
  mutate(index_val = index_val+1,
         rows = rows+1)

link_right <- edge_list_base %>%
  mutate(index_val = index_val+28,
         cols = cols+1)

link_left <- edge_list_base %>%
  mutate(index_val = index_val-28,
         cols = cols-1)


test <-bind_rows(link_above, link_below, link_left, link_right) %>%
  filter(index_val %in% 1:784,
         rows %in% 1:28,
         cols %in% 1:28)


#edges in lattice 
27^2*4 + 26*4*2

mnist_example %>%
pull(value) %>%
  matrix(data =., nrow = 28, byrow = T) %>%
  graph_from_adjacency_matrix(., mode = "undirected", weighted = TRUE)


mnist_example %>%
  ggplot(aes(y = cols, x = rows, fill = value)) +geom_raster()


```


#The position of individual nodes 

This chunk shows that individual nodes naturally cluster by elevation with thier own sub groups sometimes the tension is also needed. I am not sure how to extract this without using a model though
```{r}
#There is possibly a problem with the node position code. The part that created detected_communities is producing 40k lines intead of 20k, this may or may not be a problem...

#....The 40k lines are becuase both groups a and be are checked.

source(file = file.path(SubcodeFolder, "node_position.R"))
 
#Use ARI to compare the quality of all clustering techniques
combos_df <- expand_grid(names = detected_communities %>% select(contains("clustering")) %>% names, graph_id = unique(detected_communities$graph_id))
 
        test_ARI <-1:nrow(combos_df) %>%
          map_df(~{
            print(.x)
            temp <-detected_communities %>%
              filter(graph_id == combos_df$graph_id[.x])
            
            tibble(
              name = combos_df$names[.x],
              graph_id = combos_df$graph_id[.x],
              graph_class = unique(temp$graph_class),
              ARI = adjustedRandIndex(temp$sub_class, temp %>% pull(combos_df$names[.x]) ))
            
          })
        
        #plot the results
        #The ability to cluster graphs that have communities or groups that are not defined by their linking to each other greatly outstrips everything else
 
         #plot and example of the true communities
detected_communities %>%
  filter(graph_id %in% seq(100, 500, by = 100)) %>%
  ggplot(aes(x = tension_mean_range, y = elevation_range, colour = sub_class)) + geom_point() +
  facet_wrap(~graph_class) +
  labs(title = "The position of individual nodes in each graph type of Peel's Quintet",
       x = "node tension",
       y = "node elevation",
       color = "sub class")
  ggsave(file.path(FiguresFolder,  "peels_quintet_groups_in_setse_space.pdf"))   
 

 #plot showing how SETSe does the comparisons
 detected_communities %>%
    filter(graph_id %in% seq(100, 500, by = 100)) %>%
  ggplot(aes(x = tension_mean, y = elevation, colour = factor(clustering_kmeans))) + geom_point() +
  facet_wrap(~graph_class, scales = "free_y") +
   labs(title = "the position of each node in a graph coloured by kmeans groups",
        colour = "kmeans cluster")

              
        #when using knowlegde of the groups
        test_ARI %>%
          filter(!grepl("full", name)) %>%
          mutate(clustering = name %>% str_remove(., "clustering_") %>% str_replace(., "_", " "),
                 clustering = ifelse(clustering =="kmeans", "SETSe kmeans", clustering)) %>%
          ggplot(aes(x = clustering, y = ARI, fill = clustering)) + geom_boxplot() +
          facet_wrap(~graph_class) +
          labs(title = "Adjusted rand index for different clustering methods, for each graph type",
               x = "Clustering method") + 
          theme(
            axis.text.x=element_blank(),
            axis.ticks.x=element_blank())
    ggsave(file.path(FiguresFolder,  "peels_clustering_ari.pdf"))        
        
         #when not using knowledge of the groups
         test_ARI %>%
          filter(grepl("full", name))  %>%
          mutate(clustering = name %>% str_remove(., "clustering_") %>% str_replace(., "_", " "),
                 clustering = ifelse(clustering =="kmeans", "SETSe kmeans", clustering)) %>%
          ggplot(aes(x = clustering, y = ARI, fill = clustering)) + geom_boxplot() +
          facet_wrap(~graph_class) +
          labs(title = "Adjusted rand index for different clustering methods, for each graph type")

         #
 test <- test_ARI %>%
    group_by(name, graph_class) %>%
    summarise(mean = mean(ARI))
```

#one vs all 

This section see' what happens when you do a one vs all test with all nodes. Do they produce sub class patterns? or does some other thing occur? Does this indicate nodal influence in a graph? If so that could lead to conflict outcome prediction

This raises the question of what happens if some of the nodes alligaiances are known... This leads on to the relgious debate networks.
```{r}


1:500 %>% walk(~{

  graph_ref <- .x
  target_file <- file.path(PLwd, "peel_influence", paste0("graph_ref_", graph_ref, ".rds") )
  
  if(!file.exists(target_file)){
    print(.x)

  g_out <- multi_quintet[[graph_ref]]
  
 Out  <- one_vs_all_SETSe(multi_quintet[[graph_ref]], k_options2)  
 
 saveRDS(Out, file = target_file)
  }
})

    

test <- 1:500 %>% 
  map_df(~{
      #I need to do this for all examples
      df <- read_rds(file.path(PLwd, "peel_influence2", paste0("graph_ref_", .x, ".rds") )) 
   df %>% 
      select(contains("clustering")) %>%
          map_dbl(~{
            
            adjustedRandIndex(df$sub_class, .x)
            
          }) %>% tibble(names = names(.), values = .) %>%
      mutate(graph = .x)

})


#The algorithm is substantially worse when using a completely unsupervised approach
#but why are all the other methods exactly the same each time?
test %>% 
  mutate(graphtype = case_when(
    graph<=100 ~"A",
    graph<=200 ~"B",
    graph<=300~"C",
    graph<=400~"D",
    TRUE ~"E"
  )) %>%
 # filter(names =="clustering_walktrap") %>%
  ggplot(aes(x = names, y = values, fill = names)) +
  geom_boxplot() +
  facet_wrap(~graphtype) +
  theme(axis.text.x = element_blank()) +
  labs(title = "ARI performance of clustering techniques when no group affiliation is given")

```

#Conflict networks

This section looks at networks that experienced conflict and how the alliagieance of the network splits
all the data is from http://vlado.fmf.uni-lj.si/pub/networks/data/ucinet/UciData.htm#sampson
```{r}
files <- list.files("/home/jonno/setse_1_data/conflict_networks", full.names = T)
```


This is the thesis associated with a collection of schools dataset.
I don't know if there is any school level information available

https://search-proquest-com.libproxy.ucl.ac.uk/docview/304516167?pq-origsite=primo

data set available from; there are some problems with it.
http://moreno.ss.uci.edu/data.html

##Medici

The traditional metrics can check what would happen if we vary the strength of the different relationships
But only SETSe can vary the beligerents

This chunk shows that although the Strozzi have more power than the Albizzi, they would have lost in a congflict against the Medici, the Albizzi who although were split against and for the medici could have overpowered them if the strength of a mariage connection was no more than twice as valuable than a business connection. After that point the Albizzi lose badly.

The traditional metrics are identical for both of the conlfict types and in fact walktrap fails to separate the beligerents for large weights of marraige ties.

```{r}

source(file = file.path(SubcodeFolder, "medici_process.R"))

AlbizziandStrozzi %>%
  filter(separate_succcess) %>%
  select(1:6, ratio) %>%
  pivot_longer(cols = 2:6, names_to = "metric", values_to = "values") %>%
    filter(metric !="ARI") %>%
  ggplot(aes(x = ratio, y = values, colour = name %>% str_remove(., "clustering_") %>% str_replace(., "_", " "), group = name)) + geom_path() +
  facet_wrap(~metric) +
  labs(title = "Medici vs Oligarchs using different power metrics",
       x = "Odds ratio of business to marriage weights",
       y = "Medici factional control",
       colour = "method") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
ggsave(file.path(FiguresFolder,  "medici_factional_controll.pdf"))       


set.seed(345)
g_medici %>%
  delete_vertices(., vertex_attr(., "name")=="PUCCI" ) %>%
  ggraph(.) + 
    geom_edge_link2( width = 0.7, edge_alpha =0.5) + 
      geom_node_point( aes(colour = party, 
                           size = ifelse(name == "MEDICI"| name =="STROZZI", 6,3))) + 
    scale_size_continuous(range = c(3, 6))+
  scale_edge_colour_viridis() +
  labs(title = "The factional alligiance of the Florentine families network") +
  guides( size = "none")+
geom_node_text(aes(label = name), nudge_y = .08)
ggsave(file.path(FiguresFolder,  "Medici_partisan_network.pdf"))

#the elevation and groups plot
# medici_strozzi$groups %>%
#   filter(marriage_weight == 300, business_weight == 100) %>%
#   ggplot(aes(x = node, y = elevation, colour = factor(party),
#              size = ifelse(node == "MEDICI"| node =="STROZZI", 6,3))) + geom_point() +
#   scale_size_continuous(range = c(3, 6))+
#   guides( size = "none") + 
#   theme(axis.text.x = element_text(angle = 25, hjust = 1))+
#   labs(title = "Medici vs Strozzi a tug of war with the families of Florence",
#        x = "Family",
#        colour = "Faction")

```

##Kapferer Mine

multiplex ties can be considered strong ties and uniplex considered weak ties.

Jackson is the boss
Lotson is the shop steward

Donald and Able worked as a pair
Abraham and Benson worked as a pair


What we learn from this dataset is that the imoprtance of the difference in weighting becomes explicit in SETSe whilst it is implicit in the the traditional clustering metrics. As such the weighting can be learned through multiple observations of interactions within graph structures, this weighting can then be deployed on new observations. This is beyond the scope of this paper!

Using a weighting of from 1to1 to 100to1 we find that the meta weight edges is an important concept to consider. Abraham does not overpower Donald in the full netowrk until multi edges are worth 5 times more than uniplex edges. This is important as edges of a multiplex network are often considered equally weighted but this may not be the case, as an example regular work contact is not necessarily as valuable in conflict as regular friendly contact.

Irrespective of the relation between uni adn multi edges Benson could never overcome Abel. What's more it required a higher weighting to for Abraham to overcome Abel, it thus appears the strategically most sensible move for Abraham to accuse Donald.

This information is not possible in a normal community detection algorithm which would always break the network into identical communities. The edge weights can be changed but this assumption has to be explicity stated using SETSe wheras the assumption is implicit in the other algorithms.

```{r}

source(file = file.path(SubcodeFolder, "kapferer_process.R"))
#SETSE separates the beliggerants everytime as expected, however the other methods can only achieve this on certain parameter settings. That said they do sucessfully show that the Abraham would have beet Donald. They are much less 
Kapferer_aggregated %>%
  left_join(kapferer_separation) %>%
  filter(value == "Abraham"| value == "Benson", 
        # cluster_method == "SETSe_kmeans",
        separate_succcess,
        cluster_method != "SETSe_kmeans"
         ) %>%
  ggplot(aes(x = ratio, y = fract, colour =str_remove(cluster_method, "clustering_") %>% str_replace(., "_", " "))) + geom_line() +
  coord_cartesian(ylim = c(0,1)) +
  facet_wrap( ~combo)+
  labs(title = "Resolution of the Kapferer mine conflict on graphs of different weights",
       y = "fraction of workers who side with Abraham/Benson",
       x = "The value of multiple edges compared to a single edge",
       colour = "method")
ggsave(file.path(FiguresFolder,  "Kapferer_mine_results.pdf"))

test <-Kapferer_aggregated %>%
  group_by(combo, 
           ratio,
           cluster_method) %>%
  summarise(separation_failure = sum(is.na(value)))


spec_res <- Kapferer_embeddings_results %>%
  filter(combo  == "Abraham-Abel",
         ratio ==10)

spec_res %>%
  ggplot(aes(x = node, y = elevation, colour = SETSe_kmeans,
              size = ifelse(node == "MEDICI"| node =="STROZZI", 6,3))) + geom_point() +
  scale_size_continuous(range = c(3, 6))+
  guides( size = "none") + 
  theme(axis.text.x = element_text(angle = 25, hjust = 1))

as_data_frame(kapferer_mine_g) %>%
  graph_from_data_frame(., directed = FALSE, vertices = spec_res ) %>%
    ggraph(.) + 
    geom_edge_link2( width = 0.7, edge_alpha =0.5) + 
    geom_node_point( aes(
      colour = SETSe_kmeans,
      size = ifelse(name == "Abraham"| name =="Donald", 6,3))
    )  +   
  scale_size_continuous(range = c(3, 6))+
    scale_edge_colour_viridis() +
    labs(title = "Kapferer mine conflict") +
    guides( size = "none")+
    geom_node_text(aes(label = name), nudge_y = .08)
  


```

##Thurman Office

A whole host of crazy happened here this is to test what I can do with it
```{r}
file_id <- grepl("thuroff", files)

emma_vs_ann <- thurman_office_function("ANN", "EMMA")
pete_vs_minna <-  thurman_office_function("PETE", "MINNA")
emma_vs_group <- thurman_office_function("EMMA", c("ANN", "AMY", "TINA", "KATY","LISA"))


test <- emma_vs_group$groups

bind_rows(emma_vs_group$summary %>% mutate(model = "emma vs group"),
          emma_vs_ann$summary %>% mutate(model = "ann vs emma"),
          pete_vs_minna$summary %>%mutate(model = "pete vs minna")
            
            ) %>%
  mutate(name = str_remove(name, "clustering_" )%>% str_replace(., "_", " ")) %>%
  ggplot(aes(x = ratio, y = fract, colour = name, group = name)) + geom_line() +
  facet_wrap(~model) +
  labs(title = "Predicted control using SETSe and traditional clustering methods", 
       x ="Odds ratio of business to personal weights",
       y = " fraction of nodes controlled") +
    theme(axis.text.x = element_text(angle = 55, hjust = 1))
ggsave(file.path(FiguresFolder,  "thurman_factional_control.pdf"))       

emma_vs_group$groups %>%
ggplot(aes(y = elevation, x = node, colour = elevation>0)) + geom_point()

emma_vs_ann$groups %>%
ggplot(aes(y = elevation, x = node)) + geom_point() +
    theme(axis.text.x = element_text(angle = 25, hjust = 1))


emma_vs_group$groups %>%
  filter(node=="PRESIDENT")

g_out %>%
ggraph(.) + 
    geom_edge_link2( width = 0.7, edge_alpha =0.5) + 
      geom_node_point( aes(
                           size = ifelse(name == "PETE"| name =="MINNA", 6,3))) + 
    scale_size_continuous(range = c(3, 6))+
  scale_edge_colour_viridis() +
  labs(title = "The factional allegiance of the Florentine families network") +
  guides( size = "none")+
geom_node_text(aes(label = name), nudge_y = .08)

```


#Sampson data
```{r}
g_dl <- load_dl_graph(files[7], directed = FALSE)


plot(g_dl[[3]])

names(g_dl)

```

#Dynamic clustering

Dynamic clustering finds the most tens edge and removes it iterateivly until the required number of group are identified

It is not a fast method and is distinct from the elevation method

```{r}

```


#Dominance
taken from http://moreno.ss.uci.edu/data.html#bison

wolves are deference in the row not dominance

```{r}
files <- list.files("/home/jonno/setse_1_data/conflict_networks", full.names = T)

dominance_files <-list.files( "/home/jonno/setse_1_data/dominance_networks", full.names = T)


dominance_files
#plot graph
file_id <- grepl("bison", dominance_files)

g_dl <- load_dl_graph(dominance_files[file_id], directed = FALSE, return_graph = TRUE) 


g_dl %>%
  ggraph(.)+
      geom_edge_fan(aes(colour = weights)) +
      geom_node_point(aes(), size=3) +
     scale_edge_colour_viridis()

file_id <- grepl("bison", dominance_files)

g_dl <- load_dl_graph(dominance_files[file_id], directed = FALSE, return_graph = TRUE) %>%
  simplify

g_mat <- load_dl_graph(dominance_files[file_id], directed = TRUE, return_graph = FALSE)

bison_dominance <- domination_function(g_dl, g_mat)

node_win_loss_df <- tibble(node = g_mat %>% pull(from), node_wins = g_mat %>% select(-from) %>% rowSums()) %>%
      left_join(tibble(node = g_mat %>% select(-from) %>% names, node_losses = g_mat %>% select(-from) %>% colSums()),
                by = "node") %>%
      mutate(node_ratio = node_wins/(node_wins + node_losses))

bison_performance <- dominance_performance(g_dl,g_mat, bison_dominance)  %>% #add in total conlicts per edge/node pair
      left_join(node_win_loss_df %>% select(winner = node, winner_node_ratio = node_ratio), by = "winner" ) %>%
      left_join(node_win_loss_df %>% select(loser = node, loser_node_ratio = node_ratio), by = "loser" ) %>%
      mutate(relative_win_ratio = winner_node_ratio/loser_node_ratio,
             naive_res = case_when(
               relative_win_ratio>=1 & win_ratio>=0.5 ~"correct",
               relative_win_ratio<=1 & win_ratio<=0.5 ~"correct",
               TRUE ~"error"
             ))

bison_performance %>%
  ggplot(aes(x = log10(winner_tension/loser_tension) , y = ratio_euc, colour = win_ratio>0.5)) + geom_point()

table(bison_performance$class2)
table(bison_performance$naive_res)

g_dl <- load_dl_graph(dominance_files[2], directed = FALSE, return_graph = TRUE) %>%
  simplify

g_mat <- load_dl_graph(dominance_files[2], directed = TRUE, return_graph = FALSE)

cattle_dominance <- domination_function(g_dl, g_mat)

cattle_performance <- dominance_performance(g_mat, cattle_dominance)

#pony
g_dl <- load_dl_graph(dominance_files[3], directed = FALSE, return_graph = TRUE) %>%
  simplify

g_mat <- load_dl_graph(dominance_files[3], directed = TRUE, return_graph = FALSE)

pony_dominance <- domination_function(g_dl, g_mat)

pony_performance <- dominance_performance(g_mat, pony_dominance)


pony_dominance %>%
  ggplot(aes(x = factor(node), y = euc)) + geom_point()
    

graph_from_data_frame(as_data_frame(g_dl), directed = FALSE, vertices = bison_dominance$node) %>%
  ggraph(.) +
      geom_edge_fan() +
      geom_node_point(aes( colour = rank1), size=3) +
   scale_colour_viridis_c(direction = -1) 



test <- dominance_xvalidation(dominance_files[1], folds = 2, seed = 10) 


#Q if A is higher than B and A and B have conflict does A generally dominate?


test <-random_removal %>%
  filter(.metric =="bal_accuracy",
         cutoff ==1) 

random_removal 
testtest %>%
  filter(.metric =="bal_accuracy")  %>%
  ggplot(aes(x = cutoff , y = .estimate)) + geom_point()+
  facet_wrap(~seed) +
  geom_vline(xintercept = 1)



random_removal %>%
  filter(.metric =="bal_accuracy",
         cutoff>0.75,
         cutoff<1.5) %>%
  group_by(cutoff) %>%
  summarise(.estimate = mean(.estimate)) %>%
  ggplot(aes(x = factor(cutoff) , y = .estimate)) + geom_boxplot()

pony_performance %>%
  ggplot(aes(x = total, y =ratio_euc, colour = class)) + 
  geom_point()


#model accuracy is maximised when the ratio of 1 is used as the cutoff.
#This is the bare minimum as they will literally pull in other directions so the accuracy should be high
seq(0,2, by = 0.05) %>%
  map_df(~{
    pony_performance %>%
      mutate( 
        truth = factor(total>0.5, levels = c(TRUE, FALSE)),
        estimate= factor(ratio_euc >= .x, levels = c(TRUE, FALSE))) %>%
      metrics(., truth = truth, estimate = estimate) %>%
      mutate(cutoff = .x)
    
  }) 


test_res%>%
  ggplot(aes(x = cutoff, y = .estimate))+ geom_point() +
  facet_wrap(~.metric)

#Q does heignt in matrix predict breeding success?


fold_test <- dominance_xvalidation(dominance_files[1], folds = 2, seed = 10) 
fold_test_perf <- fold_test$performance_df
fold_test_cutoff <- fold_test$model_analysis %>%
  filter(.metric == "accuracy") 


#test_10 
#test_2 <- fold_test_cutoff 

test <- bind_rows(test_10 %>% mutate(fold = 10),
          test_5 %>% mutate(fold = 5),
            test_2 %>% mutate(fold = 2)) %>%
  group_by(type, fold) %>%
  summarise(mean = mean(.estimate))

test %>%
  rename(estimate = .estimate) %>%
  select(fold, type, estimate) %>%
  pivot_wider( names_from = type, values_from = estimate)

fold_test_cutoff %>%
  filter(.metric == "accuracy") %>%
  ggplot(aes(x = .estimate, colour = type)) + geom_density()

fold_test_cutoff %>%
  filter(.metric == "accuracy") %>%
  ggplot(aes(x = type, y =  .estimate, colour = type)) + geom_boxplot()

test2 <- test$cutoff_analysis

test <- list.files("/home/jonno/setse_1_data/dominance/bison/repeats", 
                   full.names = T) %>%
  map(read_rds)  %>% transpose() %>%
  map(~{.x %>% bind_rows()})

test2<- test$performance_df 
test3 <- test$cutoff_analysis


test3 %>%
  group_by(fold, seed) %>%
  summarise(counts = n())

test_na <- test3 %>% filter(is.na(.estimate), .metric == "accuracy",
                            cutoff == 1)

#the density plot of the mean accuracy by repeat
test3 %>%
  filter(.metric =="accuracy",
         cutoff==1) %>%
  mutate(cutoff = factor(cutoff)) %>%
  ggplot(aes( x = .estimate)) + geom_density() +
  theme(axis.text.x = element_text(angle = 35, hjust = 1))


repeat_agg <- test3 %>%
  group_by(.metric, cutoff, seed) %>%
  summarise(mean = mean(.estimate),
            sd = sd(.estimate),
            median = median(.estimate)) %>%
  ungroup

repeat_agg  %>%
  filter(.metric =="accuracy",
       #  cutoff>0.8,
         cutoff==1) %>%
  mutate(cutoff = factor(cutoff)) %>%
  ggplot(aes(  x = mean)) + geom_density() +
  theme(axis.text.x = element_text(angle = 35, hjust = 1)) +
  labs(title = "Repeated cross validation mean accuracy",
       x = "Elevation ratio minimum",
       y = "mean accuracy for each k10 repeat")


test3 %>%
  filter(.metric =="accuracy",
         cutoff == "1") %>%
  pull(.estimate) %>% t.test(., alternative = "greater", mu = 0.5)

#bootstrap confidence
set.seed(13)
resample2 <- bootstraps(test3 %>% filter(.metric=="accuracy", cutoff ==1.1), times = 1000)
map_dbl(resample2$splits,
        function(x) {
          dat <- as.data.frame(x)$.estimate
          mean(dat>0.5, na.rm = T)
        })

test <- performance_df %>%
  filter(holdout) %>%
  select(winner, loser, class2, naive_res)

table(test$class2, test$naive_res)



```

#deference

```{r}
file_id <- grepl("wolf.dat", dominance_files)

g_dl <- load_dl_graph(dominance_files[file_id], directed = FALSE, return_graph = TRUE) %>%
  simplify

g_mat <- load_dl_graph(dominance_files[file_id], directed = TRUE, return_graph = FALSE) %>%
  pivot_longer(., cols = -from, names_to = "winner") %>%
  pivot_wider(., names_from = from, values_from = value) %>%
  rename(from = winner)

deference_dominace <- domination_function(g_dl, g_mat)

node_win_loss_df <- tibble(node = g_mat %>% pull(from), node_wins = g_mat %>% select(-from) %>% rowSums()) %>%
      left_join(tibble(node = g_mat %>% select(-from) %>% names, node_losses = g_mat %>% select(-from) %>% colSums()),
                by = "node") %>%
      mutate(node_ratio = node_wins/(node_wins + node_losses))

deference_performance <- dominance_performance(g_dl,g_mat, deference_dominace)  %>% #add in total conlicts per edge/node pair
      left_join(node_win_loss_df %>% select(winner = node, winner_node_ratio = node_ratio), by = "winner" ) %>%
      left_join(node_win_loss_df %>% select(loser = node, loser_node_ratio = node_ratio), by = "loser" ) %>%
      mutate(relative_win_ratio = winner_node_ratio/loser_node_ratio,
             naive_res = case_when(
               relative_win_ratio>=1 & win_ratio>=0.5 ~"correct",
               relative_win_ratio<=1 & win_ratio<=0.5 ~"correct",
               TRUE ~"error"
             ))

deference_performance_norm 
#deference_performance2 <- deference_performance
table(deference_performance2$class2)

table(deference_performance2$naive_res)


test_node <- dominance_list$node

test_edge <- dominance_list$edge
```

#highschool

This first section downloads all the data

data from
http://moreno.ss.uci.edu/data.html

errors

1 has gender incorrectly coded
48 is missing atributes and was deleted


```{r}



school_path_1 <- file.path(PLwd, "highschool/raw")
school_path_2 <- file.path(PLwd, "highschool/graphs")


file_tibble <- tibble(file_names = list.files(school_path_1 )) %>%
  mutate(id = file_names %>% str_remove(.,"comm") %>% str_remove(., "(_|\\.).+") %>% as.integer(),
         is_att = grepl("att", file_names))

unique(file_tibble$id) %>%
  walk(~{
    print(.x)
    temp <- file_tibble %>%
      filter(id==.x) %>%
      arrange(is_att)
    
    edges <- file.path(school_path_1, temp$file_names[1]) %>%
      read_csv(, skip = 4, col_names = F) %>%
  separate(col = X1, into = c("from", "to", "weight"), sep = " +") %>%
      mutate(from = trimws(from) %>% as.integer(),
             to = trimws(to) %>% as.integer(),
             weight = trimws(weight) %>% as.integer())

    nodes <- file.path(school_path, temp$file_names[2]) %>%
      read_lines(.) %>%
      as_tibble
    
    nodes <- nodes %>%
      slice((grep("DATA:", nodes$value)+1):nrow(.))%>%
      mutate(value = trimws(value)) %>%
      separate(col = value, into = c("sex", "ethnicity", "grade", "school"), sep = " +") %>%
      mutate(node = 1:nrow(.),
             school = ifelse(is.na(school), 1, school)) %>%
      select(node, everything()) %>%
      filter(complete.cases(.))
    
   g <-graph_from_data_frame(edges, directed = FALSE, vertices = nodes) 
   
   g%>%
      saveRDS(., file = file.path(school_path_2, paste0("school_", temp$id[1], ".rds")))

    
    
  })


school_info <- list.files(school_path_2, full.names = T) %>%
  map_df(~{
    
    g <- readRDS(.x)
    
    comps <-  components(g)
    
  temp <- as_data_frame(g, what = "vertices")  %>% as_tibble()
  
  eth_fract <- 0:5 %>%
    map_df(~{
      
      temp %>%
        summarise(value = sum(ethnicity==.x)/nrow(temp)) %>%
        mutate(ethnicity = paste0("eth_",.x))
      
    }) %>%
    pivot_wider(., names_from = ethnicity, values_from = value) %>%
    bind_cols(      temp %>%
        summarise(eth_err = sum(ethnicity > 5)/nrow(temp)) )
  
  grade_fract <- temp %>%
        summarise(grade_valid = sum(grade %in% 7:12)/nrow(temp))
  
  
  tibble(file = basename(.x), 
         graph_id = file %>% str_remove(., "school_") %>% str_remove(., ".rds") %>% as.integer(),
         students = nrow(temp), 
         components = comps$no,
         schools = length(unique(temp$school)),
         edge_ratio = ecount(g)/vcount(g),
         sex_assort = assortativity(g, vertex_attr(g, "sex")),
         eth_assort = assortativity(g, vertex_attr(g, "ethnicity")),
         grade_assort = assortativity(g, vertex_attr(g, "grade")),
         sch_assort = assortativity(g, vertex_attr(g, "school")),
         fract_largest_comp = max(comps$csize)/students,
         sex_1 = sum(temp$sex==1)/students,
         sex_2 = sum(temp$sex==2)/students) %>%
    bind_cols(eth_fract, grade_fract)
  
  })





#no pattern in edge density
school_info %>%
  filter(id != 1) %>%
  ggplot(aes(x = students, y = edge_ratio, colour = factor(schools))) + geom_point()


school_info %>%
 # filter(schools ==2) %>%
  select(contains("assort")) %>%
  pivot_longer(cols = everything()) %>%
    ggplot(aes(x = value, color = name)) + geom_density()



list.files(school_path_2, full.names = T)


school_info %>%
  filter(schools ==2) %>%
  pull(file) %>% 
  walk(~{
    
    graph_name <- .x %>% str_remove(., "school_") %>% str_remove(., ".rds") %>% as.integer()
    
     out <- readRDS(file.path(school_path_2,.x)) %>%
       set_vertex_attr(., "node_names", value = 1:vcount(.)) %>%
       remove_small_components() %>%
  set_edge_attr(., "k", value = edge_attr(., "weight")*100) %>%
        prepare_SETSe_binary(., node_names = node_names, k = NULL, 
                             force_var = "school", positive_value = 1) %>%
       SETSe_bicomp(tol =2e-4, 
                      verbose = F) %>%
    map(~{ .x %>% 
        mutate(graph_id = graph_name)})
     
     saveRDS(out, file.path(PLwd, "highschool", "embeds_school", .x))
    
  })


test <- list.files(file.path(PLwd, "highschool", "embeds_school")) %>%
  map_df(~{
     
    temp <- read_rds(file.path(PLwd, "highschool", "embeds_school", .x))
    
    edge_vals <- temp$edge_embeddings %>%
      separate(col = "edge_name", into = c("from", "to"), sep = "-") %>%
      select(from, to, tension, strain) %>%
      pivot_longer(., cols = c(from, to), values_to ="node") %>%
      group_by(node) %>%
      summarise(tension = mean(tension),
                strain = mean(strain))
    
    g_df <- read_rds(file.path(PLwd, "highschool", "graphs", .x)) %>%
      as_data_frame(., what = "vertices") %>% rename(node = name)
    
    
    out <- left_join(temp$node_embeddings %>%select(node, elevation, graph_id), edge_vals, by = "node") %>%
      left_join(g_df , by = "node")
    
   return(out)
    
  }) %>%
  filter(!is.na(tension)) %>%
  group_by(graph_id) %>%
  summarise(
    strain = mean(strain),
    elevation = mean(abs(elevation)),
    tension = mean(tension)
          ) %>%
  left_join(school_info, by = "graph_id")



test %>%
  ggplot(aes(x = (sch_assort), y = log(elevation), color = sqrt(sch_assort))) + geom_point()+
  scale_color_viridis_c()

  lm(sch_assort ~ elevation + tension , test %>% mutate(elevation =log(elevation))) %>% summary()

chem <- read_csv("/home/jonno/Downloads/BradleyMeltingPointDatasetClean.csv")

```

This package might be able to read smiles strings into R.
I don't know how they would then be conveted to igraph objects though
https://bioconductor.org/packages/devel/bioc/vignettes/ChemmineR/inst/doc/ChemmineR.html


